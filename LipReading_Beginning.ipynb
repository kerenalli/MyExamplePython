{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LipReading.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3+Qhey8QLj6KjaxGN1caS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kerenalli/MyExamplePython/blob/main/LipReading_Beginning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qVf28jM9A_w",
        "outputId": "8f388be2-eb76-4813-b7f2-0d18f60e882e"
      },
      "source": [
        "pip install sk-video"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sk-video\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/3f/ce848b8b2062ad1ccf1449094a740c775f6c761339f411e44f1e090f23a7/sk_video-1.1.10-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sk-video) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sk-video) (1.19.4)\n",
            "Installing collected packages: sk-video\n",
            "Successfully installed sk-video-1.1.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9terQTdl8Qvv"
      },
      "source": [
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import dlib\r\n",
        "import math\r\n",
        "import sys\r\n",
        "import pickle\r\n",
        "import argparse\r\n",
        "import os\r\n",
        "import skvideo.io"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSlYEu8X9zYT"
      },
      "source": [
        "PART1: Construct the argument parse and parse the arguments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "I2_Ydlpz90Rq",
        "outputId": "341d8ac1-4e98-4066-cb29-f00a455f8710"
      },
      "source": [
        "ap = argparse.ArgumentParser()\r\n",
        "ap.add_argument(\"-i\", \"--input\", required=True,\r\n",
        "                help=\"path to input video file\")\r\n",
        "ap.add_argument(\"-o\", \"--output\", required=True,\r\n",
        "                help=\"path to output video file\")\r\n",
        "ap.add_argument(\"-f\", \"--fps\", type=int, default=30,\r\n",
        "                help=\"FPS of output video\")\r\n",
        "ap.add_argument(\"-c\", \"--codec\", type=str, default=\"MJPG\",\r\n",
        "                help=\"codec of output video\")\r\n",
        "args = vars(ap.parse_args())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] -i INPUT -o OUTPUT [-f FPS] [-c CODEC]\n",
            "ipykernel_launcher.py: error: argument -f/--fps: invalid int value: '/root/.local/share/jupyter/runtime/kernel-2a2e324f-4e4b-4fec-b146-91470d9c1798.json'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQI4zd53-FcN"
      },
      "source": [
        "PART2: Calling and defining required parameters for:\r\n",
        "\r\n",
        "       1 - Processing video for extracting each frame.\r\n",
        "       2 - Lip extraction from frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b6qsYpT-GbT"
      },
      "source": [
        "# Dlib requirements.\r\n",
        "predictor_path = '/content/shape_predictor_68_face_landmarks.dat'\r\n",
        "detector = dlib.get_frontal_face_detector()\r\n",
        "predictor = dlib.shape_predictor(predictor_path)\r\n",
        "mouth_destination_path = os.path.dirname(args[\"output\"]) + '/' + 'mouth'\r\n",
        "if not os.path.exists(mouth_destination_path):\r\n",
        "    os.makedirs(mouth_destination_path)\r\n",
        "\r\n",
        "inputparameters = {}\r\n",
        "outputparameters = {}\r\n",
        "reader = skvideo.io.FFmpegReader(args[\"input\"],\r\n",
        "                inputdict=inputparameters,\r\n",
        "                outputdict=outputparameters)\r\n",
        "video_shape = reader.getShape()\r\n",
        "(num_frames, h, w, c) = video_shape\r\n",
        "print(num_frames, h, w, c)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPZGRFTL_CAB"
      },
      "source": [
        "# The required parameters\r\n",
        "activation = []\r\n",
        "max_counter = 150\r\n",
        "total_num_frames = int(video_shape[0])\r\n",
        "num_frames = min(total_num_frames,max_counter)\r\n",
        "counter = 0\r\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUxYnCO0_EZc"
      },
      "source": [
        "# Define the writer\r\n",
        "writer = skvideo.io.FFmpegWriter(args[\"output\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXh8WnLO_L0M"
      },
      "source": [
        "# Required parameters for mouth extraction.\r\n",
        "width_crop_max = 0\r\n",
        "height_crop_max = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR_X8IXo_QDj"
      },
      "source": [
        "'''\r\n",
        "Processing parameters.\r\n",
        "\r\n",
        "    activation: set to one if the full mouth can be extracted and set to zero otherwise.\r\n",
        "    max_counter: How many frames will be processed.\r\n",
        "    total_num_frames: Total number of frames for the video.\r\n",
        "    num_frames: The number of frames which are subjected to be processed.\r\n",
        "    counter: The frame counter.\r\n",
        "'''\r\n",
        "\r\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUizoVbS_NMb"
      },
      "source": [
        "\"\"\"\r\n",
        "PART3: Processing the video.\r\n",
        "\r\n",
        "Procedure:\r\n",
        "     1 - Extracting each frame.\r\n",
        "     2 - Detect the mouth in the frame.\r\n",
        "     3 - Define a boarder around the mouth.\r\n",
        "     4 - Crop and save the mouth.\r\n",
        "\r\n",
        "Technical considerations:\r\n",
        "     * - For the first frame the mouth is detected and by using a boarder the mouth is extracted and cropped.\r\n",
        "     * - After the first frame the size of the cropped windows remains fixed unless for the subsequent frames\r\n",
        "          a bigger windows is required. In such a case the windows size will be increased and it will be held\r\n",
        "          fixed again unless increasing the size becoming necessary again too.\r\n",
        "\"\"\"\r\n",
        "# Loop over all frames.\r\n",
        "for frame in reader.nextFrame():\r\n",
        "    print('frame_shape:', frame.shape)\r\n",
        "\r\n",
        "    # Process the video and extract the frames up to a certain number and then stop processing.\r\n",
        "    if counter > num_frames:\r\n",
        "        break\r\n",
        "\r\n",
        "    # Detection of the frame\r\n",
        "    frame.setflags(write=True)\r\n",
        "    detections = detector(frame, 1)\r\n",
        "\r\n",
        "    # 20 mark for mouth\r\n",
        "    marks = np.zeros((2, 20))\r\n",
        "\r\n",
        "    # All unnormalized face features.\r\n",
        "    Features_Abnormal = np.zeros((190, 1))\r\n",
        "\r\n",
        "    # If the face is detected.\r\n",
        "    print(len(detections))\r\n",
        "    if len(detections) > 0:\r\n",
        "        for k, d in enumerate(detections):\r\n",
        "\r\n",
        "            # Shape of the face.\r\n",
        "            shape = predictor(frame, d)\r\n",
        "\r\n",
        "            co = 0\r\n",
        "            # Specific for the mouth.\r\n",
        "            for ii in range(48, 68):\r\n",
        "                \"\"\"\r\n",
        "                This for loop is going over all mouth-related features.\r\n",
        "                X and Y coordinates are extracted and stored separately.\r\n",
        "                \"\"\"\r\n",
        "                X = shape.part(ii)\r\n",
        "                A = (X.x, X.y)\r\n",
        "                marks[0, co] = X.x\r\n",
        "                marks[1, co] = X.y\r\n",
        "                co += 1\r\n",
        "\r\n",
        "            # Get the extreme points(top-left & bottom-right)\r\n",
        "            X_left, Y_left, X_right, Y_right = [int(np.amin(marks, axis=1)[0]), int(np.amin(marks, axis=1)[1]),\r\n",
        "                                                int(np.amax(marks, axis=1)[0]),\r\n",
        "                                                int(np.amax(marks, axis=1)[1])]\r\n",
        "\r\n",
        "            # Find the center of the mouth.\r\n",
        "            X_center = (X_left + X_right) / 2.0\r\n",
        "            Y_center = (Y_left + Y_right) / 2.0\r\n",
        "\r\n",
        "            # Make a boarder for cropping.\r\n",
        "            border = 30\r\n",
        "            X_left_new = X_left - border\r\n",
        "            Y_left_new = Y_left - border\r\n",
        "            X_right_new = X_right + border\r\n",
        "            Y_right_new = Y_right + border\r\n",
        "\r\n",
        "            # Width and height for cropping(before and after considering the border).\r\n",
        "            width_new = X_right_new - X_left_new\r\n",
        "            height_new = Y_right_new - Y_left_new\r\n",
        "            width_current = X_right - X_left\r\n",
        "            height_current = Y_right - Y_left\r\n",
        "\r\n",
        "            # Determine the cropping rectangle dimensions(the main purpose is to have a fixed area).\r\n",
        "            if width_crop_max == 0 and height_crop_max == 0:\r\n",
        "                width_crop_max = width_new\r\n",
        "                height_crop_max = height_new\r\n",
        "            else:\r\n",
        "                width_crop_max += 1.5 * np.maximum(width_current - width_crop_max, 0)\r\n",
        "                height_crop_max += 1.5 * np.maximum(height_current - height_crop_max, 0)\r\n",
        "\r\n",
        "            # # # Uncomment if the lip area is desired to be rectangular # # # #\r\n",
        "            #########################################################\r\n",
        "            # Find the cropping points(top-left and bottom-right).\r\n",
        "            X_left_crop = int(X_center - width_crop_max / 2.0)\r\n",
        "            X_right_crop = int(X_center + width_crop_max / 2.0)\r\n",
        "            Y_left_crop = int(Y_center - height_crop_max / 2.0)\r\n",
        "            Y_right_crop = int(Y_center + height_crop_max / 2.0)\r\n",
        "            #########################################################\r\n",
        "\r\n",
        "            # # # # # Uncomment if the lip area is desired to be rectangular # # # #\r\n",
        "            # #######################################\r\n",
        "            # # Use this part if the cropped area should look like a square.\r\n",
        "            # crop_length_max = max(width_crop_max, height_crop_max) / 2\r\n",
        "            #\r\n",
        "            # # Find the cropping points(top-left and bottom-right).\r\n",
        "            # X_left_crop = int(X_center - crop_length_max)\r\n",
        "            # X_right_crop = int(X_center + crop_length_max)\r\n",
        "            # Y_left_crop = int(Y_center - crop_length_max)\r\n",
        "            # Y_right_crop = int(Y_center + crop_length_max)\r\n",
        "            #########################################\r\n",
        "\r\n",
        "            if X_left_crop >= 0 and Y_left_crop >= 0 and X_right_crop < w and Y_right_crop < h:\r\n",
        "                mouth = frame[Y_left_crop:Y_right_crop, X_left_crop:X_right_crop, :]\r\n",
        "\r\n",
        "                # Save the mouth area.\r\n",
        "                mouth_gray = cv2.cvtColor(mouth, cv2.COLOR_RGB2GRAY)\r\n",
        "                cv2.imwrite(mouth_destination_path + '/' + 'frame' + '_' + str(counter) + '.png', mouth_gray)\r\n",
        "\r\n",
        "                print(\"The cropped mouth is detected ...\")\r\n",
        "                activation.append(1)\r\n",
        "            else:\r\n",
        "                cv2.putText(frame, 'The full mouth is not detectable. ', (30, 30), font, 1, (0, 255, 255), 2)\r\n",
        "                print(\"The full mouth is not detectable. ...\")\r\n",
        "                activation.append(0)\r\n",
        "\r\n",
        "    else:\r\n",
        "        cv2.putText(frame, 'Mouth is not detectable. ', (30, 30), font, 1, (0, 0, 255), 2)\r\n",
        "        print(\"Mouth is not detectable. ...\")\r\n",
        "        activation.append(0)\r\n",
        "\r\n",
        "\r\n",
        "    if activation[counter] == 1:\r\n",
        "        # Demonstration of face.\r\n",
        "        cv2.rectangle(frame, (X_left_crop, Y_left_crop), (X_right_crop, Y_right_crop), (0, 255, 0), 2)\r\n",
        "\r\n",
        "    # cv2.imshow('frame', frame)\r\n",
        "    print('frame number %d of %d' % (counter, num_frames))\r\n",
        "\r\n",
        "    # write the output frame to file\r\n",
        "    print(\"writing frame %d with activation %d\" % (counter + 1, activation[counter]))\r\n",
        "    writer.writeFrame(frame)\r\n",
        "    counter += 1\r\n",
        "\r\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE6B4JZ0_wIH"
      },
      "source": [
        "\r\n",
        "\"\"\"\r\n",
        "PART4: Save the activation vector as a list.\r\n",
        "\r\n",
        "The python script for loading a list:\r\n",
        "    with open(the_filename, 'rb') as f:\r\n",
        "        my_list = pickle.load(f)\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "the_filename = os.path.dirname(args[\"output\"]) + '/' + 'activation'\r\n",
        "my_list = activation\r\n",
        "with open(the_filename, 'wb') as f:\r\n",
        "    pickle.dump(my_list, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4wSFRnLAG4H"
      },
      "source": [
        "# **Audio**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNQRxTNZAWoW"
      },
      "source": [
        "######################################\r\n",
        "####### Define the dataset class #####\r\n",
        "######################################\r\n",
        "class AudioDataset():\r\n",
        "    \"\"\"Audio dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, files_path, audio_dir, transform=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            files_path (string): Path to the .txt file which the address of files are saved in it.\r\n",
        "            root_dir (string): Directory with all the audio files.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oqaT5I3_5xU"
      },
      "source": [
        "import os\r\n",
        "from scipy.io.wavfile import read\r\n",
        "import scipy.io.wavfile as wav\r\n",
        "import subprocess as sp\r\n",
        "import numpy as np\r\n",
        "import argparse\r\n",
        "import random\r\n",
        "import os\r\n",
        "import sys\r\n",
        "from random import shuffle\r\n",
        "import speechpy\r\n",
        "import datetime\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AqbQNb8AnZX"
      },
      "source": [
        "class AudioDataset():\r\n",
        "    \"\"\"Audio dataset.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, files_path, audio_dir, transform=None):\r\n",
        "        \"\"\"\r\n",
        "        Args:\r\n",
        "            files_path (string): Path to the .txt file which the address of files are saved in it.\r\n",
        "            root_dir (string): Directory with all the audio files.\r\n",
        "            transform (callable, optional): Optional transform to be applied\r\n",
        "                on a sample.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # self.sound_files = [x.strip() for x in content]\r\n",
        "        self.audio_dir = audio_dir\r\n",
        "        self.transform = transform\r\n",
        "\r\n",
        "        # Open the .txt file and create a list from each line.\r\n",
        "        with open(files_path, 'r') as f:\r\n",
        "            content = f.readlines()\r\n",
        "        # you may also want to remove whitespace characters like `\\n` at the end of each line\r\n",
        "        list_files = []\r\n",
        "        for x in content:\r\n",
        "            sound_file_path = os.path.join(self.audio_dir, x.strip().split()[1])\r\n",
        "            try:\r\n",
        "                with open(sound_file_path, 'rb') as f:\r\n",
        "                    riff_size, _ = wav._read_riff_chunk(f)\r\n",
        "                    file_size = os.path.getsize(sound_file_path)\r\n",
        "\r\n",
        "                # Assertion error.\r\n",
        "                assert riff_size == file_size and os.path.getsize(sound_file_path) > 1000, \"Bad file!\"\r\n",
        "\r\n",
        "                # Add to list if file is OK!\r\n",
        "                list_files.append(x.strip())\r\n",
        "            except OSError as err:\r\n",
        "                print(\"OS error: {0}\".format(err))\r\n",
        "            except ValueError:\r\n",
        "                print('file %s is corrupted!' % sound_file_path)\r\n",
        "            # except:\r\n",
        "            #     print(\"Unexpected error:\", sys.exc_info()[0])\r\n",
        "            #     raise\r\n",
        "\r\n",
        "        # Save the correct and healthy sound files to a list.\r\n",
        "        self.sound_files = list_files\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.sound_files)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        # Get the sound file path\r\n",
        "        sound_file_path = os.path.join(self.audio_dir, self.sound_files[idx].split()[1])\r\n",
        "\r\n",
        "        ##############################\r\n",
        "        ### Reading and processing ###\r\n",
        "        ##############################\r\n",
        "\r\n",
        "        # Reading .wav file\r\n",
        "        fs, signal = wav.read(sound_file_path)\r\n",
        "\r\n",
        "        # Reading .wav file\r\n",
        "        import soundfile as sf\r\n",
        "        signal, fs = sf.read(sound_file_path)\r\n",
        "\r\n",
        "        ###########################\r\n",
        "        ### Feature Extraction ####\r\n",
        "        ###########################\r\n",
        "\r\n",
        "        # DEFAULTS:\r\n",
        "        num_coefficient = 40\r\n",
        "\r\n",
        "        # Staching frames\r\n",
        "        frames = speechpy.processing.stack_frames(signal, sampling_frequency=fs, frame_length=0.02,\r\n",
        "                                                  frame_stride=0.02,\r\n",
        "                                                  zero_padding=True)\r\n",
        "\r\n",
        "        # # Extracting power spectrum (choosing 3 seconds and elimination of DC)\r\n",
        "        power_spectrum = speechpy.processing.power_spectrum(frames, fft_points=2 * num_coefficient)[:, 1:]\r\n",
        "\r\n",
        "        logenergy = speechpy.feature.lmfe(signal, sampling_frequency=fs, frame_length=0.02, frame_stride=0.02,\r\n",
        "                                          num_filters=num_coefficient, fft_length=1024, low_frequency=0,\r\n",
        "                                          high_frequency=None)\r\n",
        "        \r\n",
        "\r\n",
        "        ########################\r\n",
        "        ### Handling sample ####\r\n",
        "        ########################\r\n",
        "\r\n",
        "        # Label extraction\r\n",
        "        label = int(self.sound_files[idx].split()[0])\r\n",
        "\r\n",
        "        sample = {'feature': logenergy, 'label': label}\r\n",
        "\r\n",
        "        ########################\r\n",
        "        ### Post Processing ####\r\n",
        "        ########################\r\n",
        "        if self.transform:\r\n",
        "            sample = self.transform(sample)\r\n",
        "        else:\r\n",
        "            feature, label = sample['feature'], sample['label']\r\n",
        "            sample = feature, label\r\n",
        "\r\n",
        "        return sample\r\n",
        "        # return sample\r\n",
        "\r\n",
        "\r\n",
        "class CMVN(object):\r\n",
        "    \"\"\"Cepstral mean variance normalization.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        feature, label = sample['feature'], sample['label']\r\n",
        "\r\n",
        "        # Mean variance normalization of the spectrum.\r\n",
        "        # The following line should be Uncommented if cepstral mean variance normalization is desired!\r\n",
        "        feature = speechpy.processing.cmvn(feature, variance_normalization=True)\r\n",
        "\r\n",
        "        return {'feature': feature, 'label': label}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK1QJ6cRAukd"
      },
      "source": [
        "class Extract_Derivative(object):\r\n",
        "    \"\"\"\r\n",
        "    Extract derivative features.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        feature, label = sample['feature'], sample['label']\r\n",
        "\r\n",
        "        # Extract derivative features\r\n",
        "        feature = speechpy.feature.extract_derivative_feature(feature)\r\n",
        "\r\n",
        "        return {'feature': feature, 'label': label}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__MYgnfpAw27"
      },
      "source": [
        "class Feature_Cube(object):\r\n",
        "    \"\"\"Return a feature cube of desired size.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        cube_shape (tuple): The shape of the feature cube.\r\n",
        "        ex: cube_shape=(15,40,3)\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, cube_shape):\r\n",
        "        \r\n",
        "        self.cube_shape = cube_shape\r\n",
        "        if self.cube_shape != None:\r\n",
        "            self.num_frames = cube_shape[0]\r\n",
        "            self.num_features = cube_shape[1]\r\n",
        "            self.num_channels = cube_shape[2]\r\n",
        "\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        feature, label = sample['feature'], sample['label']         \r\n",
        "\r\n",
        "        if self.cube_shape != None:\r\n",
        "            feature_cube = np.zeros((self.num_frames, self.num_features, self.num_channels), dtype=np.float32)\r\n",
        "            feature_cube = feature[0:self.num_frames, :, :]\r\n",
        "        else:\r\n",
        "            feature_cube = feature\r\n",
        "                 \r\n",
        "        \r\n",
        "        # return {'feature': feature_cube, 'label': label}\r\n",
        "        return {'feature': feature_cube[None, :, :, :], 'label': label}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBmRLgHUA2AK"
      },
      "source": [
        "class ToOutput(object):\r\n",
        "    \"\"\"Return the output.\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __call__(self, sample):\r\n",
        "        feature, label = sample['feature'], sample['label']\r\n",
        "\r\n",
        "        return feature, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASpx2PJBA93B"
      },
      "source": [
        "class Compose(object):\r\n",
        "    \"\"\"Composes several transforms together.\r\n",
        "    Args:\r\n",
        "        transforms (list of ``Transform`` objects): list of transforms to compose.\r\n",
        "    Example:\r\n",
        "        >>> Compose([\r\n",
        "        >>>     CMVN(),\r\n",
        "        >>>     Feature_Cube(cube_shape=(20, 80, 40),\r\n",
        "        >>>     augmentation=True), ToOutput(),\r\n",
        "        >>>        ])\r\n",
        "        If necessary, for the details of this class, please refer to Pytorch documentation.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(self, transforms):\r\n",
        "        self.transforms = transforms\r\n",
        "\r\n",
        "    def __call__(self, img):\r\n",
        "        for t in self.transforms:\r\n",
        "            img = t(img)\r\n",
        "        return img\r\n",
        "\r\n",
        "    def __repr__(self):\r\n",
        "        format_string = self.__class__.__name__ + '('\r\n",
        "        for t in self.transforms:\r\n",
        "            format_string += '\\n'\r\n",
        "            format_string += '    {0}'.format(t)\r\n",
        "        format_string += '\\n)'\r\n",
        "        return format_string\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    # add parser\r\n",
        "    parser = argparse.ArgumentParser(description='Input pipeline')\r\n",
        "\r\n",
        "    # The text file in which the paths to the audio files are available.\r\n",
        "    # The path are relative to the directory of the audio files\r\n",
        "    # Format of each line of the txt file is \"class_label subject_dir/sound_file_name.ext\"\r\n",
        "    # Example of each line: 0 subject/sound.wav\r\n",
        "    parser.add_argument('--file_path',\r\n",
        "                        default=os.path.expanduser(\r\n",
        "                            '~/github/3D-convolutional-speaker-recognition/code/0-input/file_path.txt'),\r\n",
        "                        help='The file names for development phase')\r\n",
        "\r\n",
        "    # The directory of the audio files separated by subject\r\n",
        "    parser.add_argument('--audio_dir',\r\n",
        "                        default=os.path.expanduser('~/github/lip-reading-deeplearning/code/speech-input/Audio'),\r\n",
        "                        help='Location of sound files')\r\n",
        "    args = parser.parse_args()\r\n",
        "\r\n",
        "    dataset = AudioDataset(files_path=args.file_path, audio_dir=args.audio_dir,\r\n",
        "                           transform=Compose([Extract_Derivative(), Feature_Cube(cube_shape=None), ToOutput()]))\r\n",
        "    idx = 0\r\n",
        "    feature, label = dataset.__getitem__(idx)\r\n",
        "    print(feature.shape)\r\n",
        "    print(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acMojCyUA_tz"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2haG_K-nBUi6"
      },
      "source": [
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import sys\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from tensorflow.python.ops import control_flow_ops\r\n",
        "from nets import nets_factory\r\n",
        "from auxiliary import losses\r\n",
        "from roc_curve import calculate_roc\r\n",
        "import os\r\n",
        "\r\n",
        "slim = tf.contrib.slim\r\n",
        "\r\n",
        "######################\r\n",
        "# Train Directory #\r\n",
        "######################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'train_dir', os.path.expanduser('~/results/TRAIN_CNN_3D'),\r\n",
        "    'Directory where checkpoints and event logs are written to.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer('num_clones', 1,\r\n",
        "                            'Number of model clones to deploy.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\r\n",
        "                            'Use CPUs to deploy clones.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer(\r\n",
        "    'log_every_n_steps', 1,\r\n",
        "    'The frequency with which logs are print.')\r\n",
        "\r\n",
        "\r\n",
        "######################\r\n",
        "# Optimization Flags #\r\n",
        "######################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'weight_decay', 0.00004, 'The weight decay on the model weights.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'optimizer', 'adam',\r\n",
        "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\r\n",
        "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\r\n",
        "\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'adam_beta1', 0.9,\r\n",
        "    'The exponential decay rate for the 1st moment estimates.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'adam_beta2', 0.999,\r\n",
        "    'The exponential decay rate for the 2nd moment estimates.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'momentum', 0.9,\r\n",
        "    'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\r\n",
        "\r\n",
        "#######################\r\n",
        "# Learning Rate Flags #\r\n",
        "#######################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'learning_rate_decay_type',\r\n",
        "    'exponential',\r\n",
        "    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\r\n",
        "    ' or \"polynomial\"')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'end_learning_rate', 0.0001,\r\n",
        "    'The minimal end learning rate used by a polynomial decay learning rate.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'label_smoothing', 0.0, 'The amount of label smoothing.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'num_epochs_per_decay', 5.0,\r\n",
        "    'Number of epochs after which learning rate decays.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_bool(\r\n",
        "    'sync_replicas', False,\r\n",
        "    'Whether or not to synchronize the replicas during training.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer(\r\n",
        "    'replicas_to_aggregate', 1,\r\n",
        "    'The Number of gradients to collect before updating params.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'moving_average_decay', None,\r\n",
        "    'The decay to use for the moving average.'\r\n",
        "    'If left as None, then moving averages are not used.')\r\n",
        "\r\n",
        "#######################\r\n",
        "# Dataset Flags #\r\n",
        "#######################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'model_speech_name', 'lipread_speech', 'The name of the architecture to train.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'model_mouth_name', 'lipread_mouth', 'The name of the architecture to train.')\r\n",
        "\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer(\r\n",
        "    'batch_size', 32, 'The number of samples in each batch.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer(\r\n",
        "    'num_epochs', 1, 'The number of epochs for training.')\r\n",
        "\r\n",
        "\r\n",
        "#####################\r\n",
        "# Fine-Tuning Flags #\r\n",
        "#####################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'checkpoint_path', None,\r\n",
        "    'The path to a checkpoint from which to fine-tune. ex:/home/sina/TRAIN_CASIA/train_logs/vgg_19.cpkt')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'checkpoint_exclude_scopes', None,\r\n",
        "    'Comma-separated list of scopes of variables to exclude when restoring'\r\n",
        "    'from a checkpoint. ex: vgg_19/fc8/biases,vgg_19/fc8/weights')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'trainable_scopes', None,\r\n",
        "    'Comma-separated list of scopes to filter the set of variables to train.'\r\n",
        "    'By default, None would train all the variables.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_boolean(\r\n",
        "    'ignore_missing_vars', False,\r\n",
        "    'When restoring a checkpoint would ignore missing variables.')\r\n",
        "\r\n",
        "# Store all elemnts in FLAG structure!\r\n",
        "FLAGS = tf.app.flags.FLAGS\r\n",
        "\r\n",
        "\r\n",
        "def _configure_learning_rate(num_samples_per_epoch, global_step):\r\n",
        "    \"\"\"Configures the learning rate.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      num_samples_per_epoch: The number of samples in each epoch of training.\r\n",
        "      global_step: The global_step tensor.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      A `Tensor` representing the learning rate.\r\n",
        "\r\n",
        "    Raises:\r\n",
        "      ValueError: if\r\n",
        "    \"\"\"\r\n",
        "    decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\r\n",
        "                      FLAGS.num_epochs_per_decay)\r\n",
        "    if FLAGS.sync_replicas:\r\n",
        "        decay_steps /= FLAGS.replicas_to_aggregate\r\n",
        "\r\n",
        "    if FLAGS.learning_rate_decay_type == 'exponential':\r\n",
        "        return tf.train.exponential_decay(FLAGS.learning_rate,\r\n",
        "                                          global_step,\r\n",
        "                                          decay_steps,\r\n",
        "                                          FLAGS.learning_rate_decay_factor,\r\n",
        "                                          staircase=True,\r\n",
        "                                          name='exponential_decay_learning_rate')\r\n",
        "    elif FLAGS.learning_rate_decay_type == 'fixed':\r\n",
        "        return tf.constant(FLAGS.learning_rate, name='fixed_learning_rate')\r\n",
        "    elif FLAGS.learning_rate_decay_type == 'polynomial':\r\n",
        "        return tf.train.polynomial_decay(FLAGS.learning_rate,\r\n",
        "                                         global_step,\r\n",
        "                                         decay_steps,\r\n",
        "                                         FLAGS.end_learning_rate,\r\n",
        "                                         power=1.0,\r\n",
        "                                         cycle=False,\r\n",
        "                                         name='polynomial_decay_learning_rate')\r\n",
        "    else:\r\n",
        "        raise ValueError('learning_rate_decay_type [%s] was not recognized',\r\n",
        "                         FLAGS.learning_rate_decay_type)\r\n",
        "\r\n",
        "\r\n",
        "def _configure_optimizer(learning_rate):\r\n",
        "    \"\"\"Configures the optimizer used for training.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      learning_rate: A scalar or `Tensor` learning rate.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      An instance of an optimizer.\r\n",
        "\r\n",
        "    Raises:\r\n",
        "      ValueError: if FLAGS.optimizer is not recognized.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    if FLAGS.optimizer == 'adam':\r\n",
        "        optimizer = tf.train.AdamOptimizer(\r\n",
        "            learning_rate,\r\n",
        "            beta1=FLAGS.adam_beta1,\r\n",
        "            beta2=FLAGS.adam_beta2,\r\n",
        "            epsilon=FLAGS.opt_epsilon)\r\n",
        "    elif FLAGS.optimizer == 'sgd':\r\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n",
        "    else:\r\n",
        "        raise ValueError('Optimizer [%s] was not recognized', FLAGS.optimizer)\r\n",
        "    return optimizer\r\n",
        "\r\n",
        "\r\n",
        "def average_gradients(tower_grads):\r\n",
        "    \"\"\"Calculate the average gradient for each shared variable across all towers.\r\n",
        "\r\n",
        "    Note that this function provides a synchronization point across all towers.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      tower_grads: List of lists of (gradient, variable) tuples. The outer list\r\n",
        "        is over individual gradients. The inner list is over the gradient\r\n",
        "        calculation for each tower.\r\n",
        "    Returns:\r\n",
        "       List of pairs of (gradient, variable) where the gradient has been averaged\r\n",
        "       across all towers.\r\n",
        "    \"\"\"\r\n",
        "    average_grads = []\r\n",
        "    for grad_and_vars in zip(*tower_grads):\r\n",
        "        # Note that each grad_and_vars looks like the following:\r\n",
        "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n",
        "        grads = []\r\n",
        "        for g, _ in grad_and_vars:\r\n",
        "            # Add 0 dimension to the gradients to represent the tower.\r\n",
        "            expanded_g = tf.expand_dims(g, 0)\r\n",
        "\r\n",
        "            # Append on a 'tower' dimension which we will average over below.\r\n",
        "            grads.append(expanded_g)\r\n",
        "\r\n",
        "        # Average over the 'tower' dimension.\r\n",
        "        grad = tf.concat(axis=0, values=grads)\r\n",
        "        grad = tf.reduce_mean(grad, 0)\r\n",
        "\r\n",
        "        # Keep in mind that the Variables are redundant because they are shared\r\n",
        "        # across towers. So .. we will just return the first tower's pointer to\r\n",
        "        # the Variable.\r\n",
        "        v = grad_and_vars[0][1]\r\n",
        "        grad_and_var = (grad, v)\r\n",
        "        average_grads.append(grad_and_var)\r\n",
        "    return average_grads\r\n",
        "\r\n",
        "\r\n",
        "def _get_variables_to_train():\r\n",
        "    \"\"\"Returns a list of variables to train.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      A list of variables to train by the optimizer.\r\n",
        "    \"\"\"\r\n",
        "    if FLAGS.trainable_scopes is None:\r\n",
        "        return tf.trainable_variables()\r\n",
        "    else:\r\n",
        "        scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(',')]\r\n",
        "\r\n",
        "    variables_to_train = []\r\n",
        "    for scope in scopes:\r\n",
        "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\r\n",
        "        variables_to_train.extend(variables)\r\n",
        "    return variables_to_train\r\n",
        "\r\n",
        "\r\n",
        "# Definign arbitrary data\r\n",
        "num_training_samples = 1000\r\n",
        "num_testing_samples = 1000\r\n",
        "train_data = {}\r\n",
        "\r\n",
        "train_data = {'mouth': np.random.random_sample(size=(num_training_samples, 9, 60, 100, 1)),\r\n",
        "              'speech': np.random.random_sample(size=(num_training_samples, 15, 40, 1, 3))}\r\n",
        "test_data = {'mouth': np.random.random_sample(size=(num_testing_samples, 9, 60, 100, 1)),\r\n",
        "             'speech': np.random.random_sample(size=(num_testing_samples, 15, 40, 1, 3))}\r\n",
        "\r\n",
        "train_label = np.random.randint(2, size=(num_training_samples, 1))\r\n",
        "test_label = np.random.randint(2, size=(num_testing_samples, 1))\r\n",
        "\r\n",
        "\r\n",
        "# # Uncomment if data standardalization is required and the mean and std vectors have been calculated.\r\n",
        "# ############ Get the mean vectors ####################\r\n",
        "#\r\n",
        "# # mean mouth\r\n",
        "# mean_mouth = np.load('/path/to/mean/file/mouth.npy')\r\n",
        "# # mean_mouth = np.tile(mean_mouth.reshape(47, 73, 1), (1, 1, 9))\r\n",
        "# mean_mouth = mean_mouth[None, :]\r\n",
        "# mean_channel_mouth = np.mean(mean_mouth)\r\n",
        "#\r\n",
        "# # mean speech\r\n",
        "# mean_speech = np.load('/path/to/mean/file/speech.npy')\r\n",
        "# mean_speech = mean_speech[None, :]\r\n",
        "# # mean_channel_speech = np.hstack((\r\n",
        "# #     [np.mean(mean_speech[:, :, :, 0])], [np.mean(mean_speech[:, :, :, 1])], [np.mean(mean_speech[:, :, :, 2])]))\r\n",
        "#\r\n",
        "# ############ Get the std vectors ####################\r\n",
        "#\r\n",
        "# # mean std\r\n",
        "# std_mouth = np.load('/path/to/std/file/mouth.npy')\r\n",
        "# std_mouth = np.tile(std_mouth.reshape(60, 100, 1), (1, 1, 9))\r\n",
        "# std_mouth = std_mouth[None, :]\r\n",
        "#\r\n",
        "# # mean speech\r\n",
        "# std_speech = np.load('/path/to/std/file/speech.npy')\r\n",
        "# std_speech = std_speech[None, :]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def main(_):\r\n",
        "\r\n",
        "\r\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
        "\r\n",
        "    graph = tf.Graph()\r\n",
        "    with graph.as_default(), tf.device('/cpu:0'):\r\n",
        "        ######################\r\n",
        "        # Config model_deploy#\r\n",
        "        ######################\r\n",
        "\r\n",
        "        # required from data\r\n",
        "        num_samples_per_epoch = train_data['mouth'].shape[0]\r\n",
        "        num_batches_per_epoch = int(num_samples_per_epoch / FLAGS.batch_size)\r\n",
        "\r\n",
        "        num_samples_per_epoch_test = test_data['mouth'].shape[0]\r\n",
        "        num_batches_per_epoch_test = int(num_samples_per_epoch_test / FLAGS.batch_size)\r\n",
        "\r\n",
        "        # Create global_step\r\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\r\n",
        "\r\n",
        "        #########################################\r\n",
        "        # Configure the larning rate. #\r\n",
        "        #########################################\r\n",
        "        learning_rate = _configure_learning_rate(num_samples_per_epoch, global_step)\r\n",
        "        opt = _configure_optimizer(learning_rate)\r\n",
        "\r\n",
        "        ######################\r\n",
        "        # Select the network #\r\n",
        "        ######################\r\n",
        "        is_training = tf.placeholder(tf.bool)\r\n",
        "\r\n",
        "        network_speech_fn = nets_factory.get_network_fn(\r\n",
        "            FLAGS.model_speech_name,\r\n",
        "            num_classes=2,\r\n",
        "            weight_decay=FLAGS.weight_decay,\r\n",
        "            is_training=is_training)\r\n",
        "\r\n",
        "        network_mouth_fn = nets_factory.get_network_fn(\r\n",
        "            FLAGS.model_mouth_name,\r\n",
        "            num_classes=2,\r\n",
        "            weight_decay=FLAGS.weight_decay,\r\n",
        "            is_training=is_training)\r\n",
        "\r\n",
        "        #####################################\r\n",
        "        # Select the preprocessing function #\r\n",
        "        #####################################\r\n",
        "\r\n",
        "        # TODO: Do some preprocessing if necessary.\r\n",
        "\r\n",
        "        ##############################################################\r\n",
        "        # Create a dataset provider that loads data from the dataset #\r\n",
        "        ##############################################################\r\n",
        "        # with tf.device(deploy_config.inputs_device()):\r\n",
        "        \"\"\"\r\n",
        "        Define the place holders and creating the batch tensor.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # Mouth spatial set\r\n",
        "        INPUT_SEQ_LENGTH = 9\r\n",
        "        INPUT_HEIGHT = 60\r\n",
        "        INPUT_WIDTH = 100\r\n",
        "        INPUT_CHANNELS = 1\r\n",
        "        batch_mouth = tf.placeholder(tf.float32, shape=(\r\n",
        "            [None, INPUT_SEQ_LENGTH, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS]))\r\n",
        "\r\n",
        "        # Speech spatial set\r\n",
        "        INPUT_SEQ_LENGTH_SPEECH = 15\r\n",
        "        INPUT_HEIGHT_SPEECH = 40\r\n",
        "        INPUT_WIDTH_SPEECH = 1\r\n",
        "        INPUT_CHANNELS_SPEECH = 3\r\n",
        "        batch_speech = tf.placeholder(tf.float32, shape=(\r\n",
        "            [None, INPUT_SEQ_LENGTH_SPEECH, INPUT_HEIGHT_SPEECH, INPUT_WIDTH_SPEECH, INPUT_CHANNELS_SPEECH]))\r\n",
        "\r\n",
        "        # Label\r\n",
        "        batch_labels = tf.placeholder(tf.uint8, (None, 1))\r\n",
        "        margin_imp_tensor = tf.placeholder(tf.float32, ())\r\n",
        "\r\n",
        "        ################################\r\n",
        "        ## Feed forwarding to network ##\r\n",
        "        ################################\r\n",
        "        tower_grads = []\r\n",
        "        with tf.variable_scope(tf.get_variable_scope()):\r\n",
        "            for i in range(FLAGS.num_clones):\r\n",
        "                with tf.device('/gpu:%d' % i):\r\n",
        "                    with tf.name_scope('%s_%d' % ('tower', i)) as scope:\r\n",
        "                        \"\"\"\r\n",
        "                        Two distance metric are defined:\r\n",
        "                           1 - distance_weighted: which is a weighted average of the distance between two structures.\r\n",
        "                           2 - distance_l2: which is the regular l2-norm of the two networks outputs.\r\n",
        "                        Place holders\r\n",
        "\r\n",
        "                        \"\"\"\r\n",
        "                        ########################################\r\n",
        "                        ######## Outputs of two networks #######\r\n",
        "                        ########################################\r\n",
        "\r\n",
        "                        logits_speech, end_points_speech = network_speech_fn(batch_speech)\r\n",
        "                        logits_mouth, end_points_mouth = network_mouth_fn(batch_mouth)\r\n",
        "\r\n",
        "                        # # Uncomment if the output embedding is desired to be as |f(x)| = 1\r\n",
        "                        # logits_speech = tf.nn.l2_normalize(logits_speech, dim=1, epsilon=1e-12, name=None)\r\n",
        "                        # logits_mouth = tf.nn.l2_normalize(logits_mouth, dim=1, epsilon=1e-12, name=None)\r\n",
        "\r\n",
        "                        #################################################\r\n",
        "                        ########### Loss Calculation ####################\r\n",
        "                        #################################################\r\n",
        "\r\n",
        "                        # ##### Weighted distance using a fully connected layer #####\r\n",
        "                        # distance_vector = tf.subtract(logits_speech, logits_mouth,  name=None)\r\n",
        "                        # distance_weighted = slim.fully_connected(distance_vector, 1, activation_fn=tf.nn.sigmoid,\r\n",
        "                        #                                          normalizer_fn=None,\r\n",
        "                        #                                          scope='fc_weighted')\r\n",
        "\r\n",
        "                        ##### Euclidean distance ####\r\n",
        "                        distance_l2 = tf.sqrt(\r\n",
        "                            tf.reduce_sum(tf.pow(tf.subtract(logits_speech, logits_mouth), 2), 1, keepdims=True))\r\n",
        "\r\n",
        "                        ##### Contrastive loss ######\r\n",
        "                        loss = losses.contrastive_loss(batch_labels, distance_l2, margin_imp=margin_imp_tensor,\r\n",
        "                                                       scope=scope)\r\n",
        "\r\n",
        "                        # ##### call the optimizer ######\r\n",
        "                        # # TODO: call optimizer object outside of this gpu environment\r\n",
        "                        #\r\n",
        "                        # Reuse variables for the next tower.\r\n",
        "                        tf.get_variable_scope().reuse_variables()\r\n",
        "\r\n",
        "                        # Calculate the gradients for the batch of data on this CIFAR tower.\r\n",
        "                        grads = opt.compute_gradients(loss)\r\n",
        "\r\n",
        "                        # Keep track of the gradients across all towers.\r\n",
        "                        tower_grads.append(grads)\r\n",
        "\r\n",
        "\r\n",
        "        # Calculate the mean of each gradient.\r\n",
        "        grads = average_gradients(tower_grads)\r\n",
        "\r\n",
        "        # Apply the gradients to adjust the shared variables.\r\n",
        "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\r\n",
        "\r\n",
        "        # Track the moving averages of all trainable variables.\r\n",
        "        MOVING_AVERAGE_DECAY = 0.9999\r\n",
        "        variable_averages = tf.train.ExponentialMovingAverage(\r\n",
        "            MOVING_AVERAGE_DECAY, global_step)\r\n",
        "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\r\n",
        "\r\n",
        "        # Group all updates to into a single train op.\r\n",
        "        train_op = tf.group(apply_gradient_op, variables_averages_op)\r\n",
        "\r\n",
        "        #################################################\r\n",
        "        ########### Summary Section #####################\r\n",
        "        #################################################\r\n",
        "\r\n",
        "        # Gather initial summaries.\r\n",
        "        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\r\n",
        "\r\n",
        "        # Add summaries for all end_points.\r\n",
        "        for end_point in end_points_speech:\r\n",
        "            x = end_points_speech[end_point]\r\n",
        "            # summaries.add(tf.summary.histogram('activations_speech/' + end_point, x))\r\n",
        "            summaries.add(tf.summary.scalar('sparsity_speech/' + end_point,\r\n",
        "                                            tf.nn.zero_fraction(x)))\r\n",
        "\r\n",
        "        for end_point in end_points_mouth:\r\n",
        "            x = end_points_mouth[end_point]\r\n",
        "            # summaries.add(tf.summary.histogram('activations_mouth/' + end_point, x))\r\n",
        "            summaries.add(tf.summary.scalar('sparsity_mouth/' + end_point,\r\n",
        "                                            tf.nn.zero_fraction(x)))\r\n",
        "\r\n",
        "        # Add summaries for variables.\r\n",
        "        for variable in slim.get_model_variables():\r\n",
        "            summaries.add(tf.summary.histogram(variable.op.name, variable))\r\n",
        "\r\n",
        "        # Add to parameters to summaries\r\n",
        "        summaries.add(tf.summary.scalar('learning_rate', learning_rate))\r\n",
        "        summaries.add(tf.summary.scalar('global_step', global_step))\r\n",
        "        summaries.add(tf.summary.scalar('eval/Loss', loss))\r\n",
        "        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES))\r\n",
        "\r\n",
        "        # Merge all summaries together.\r\n",
        "        summary_op = tf.summary.merge(list(summaries), name='summary_op')\r\n",
        "\r\n",
        "    ###########################\r\n",
        "    ######## Training #########\r\n",
        "    ###########################\r\n",
        "\r\n",
        "    with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n",
        "\r\n",
        "        # Initialization of the network.\r\n",
        "        variables_to_restore = slim.get_variables_to_restore()\r\n",
        "        saver = tf.train.Saver(variables_to_restore, max_to_keep=20)\r\n",
        "        coord = tf.train.Coordinator()\r\n",
        "        sess.run(tf.global_variables_initializer())\r\n",
        "        sess.run(tf.local_variables_initializer())\r\n",
        "\r\n",
        "        # # Restore the model\r\n",
        "        # saver.restore(sess, '/home/sina/TRAIN_LIPREAD/train_logs-1366')\r\n",
        "\r\n",
        "        # op to write logs to Tensorboard\r\n",
        "        summary_writer = tf.summary.FileWriter(FLAGS.train_dir, graph=graph)\r\n",
        "\r\n",
        "        #####################################\r\n",
        "        ############## TRAIN ################\r\n",
        "        #####################################\r\n",
        "\r\n",
        "        step = 1\r\n",
        "        for epoch in range(FLAGS.num_epochs):\r\n",
        "\r\n",
        "            # Loop over all batches\r\n",
        "\r\n",
        "            for batch_num in range(num_batches_per_epoch):\r\n",
        "                step += 1\r\n",
        "                start_idx = batch_num * FLAGS.batch_size\r\n",
        "                end_idx = (batch_num + 1) * FLAGS.batch_size\r\n",
        "                speech_train, mouth_train, label_train = train_data['speech'][start_idx:end_idx], train_data['mouth'][\r\n",
        "                                                                                                  start_idx:end_idx], train_label[\r\n",
        "                                                                                                                      start_idx:end_idx]\r\n",
        "\r\n",
        "                # # # Standardalization for speech if necessary\r\n",
        "                # speech_train = (speech_train - mean_speech) / std_speech\r\n",
        "                #\r\n",
        "                # # # Standardalization  for visual if necessary\r\n",
        "                # mouth_train = (mouth_train - mean_mouth) / std_mouth\r\n",
        "\r\n",
        "                #########################################################################\r\n",
        "                ################## Online Pair Selection Algorithm ######################\r\n",
        "                #########################################################################\r\n",
        "                online_pair_selection = True\r\n",
        "                if online_pair_selection:\r\n",
        "                    distance = sess.run(\r\n",
        "                        distance_l2,\r\n",
        "                        feed_dict={is_training: False, batch_speech: speech_train,\r\n",
        "                                   batch_mouth: mouth_train,\r\n",
        "                                   batch_labels: label_train.reshape([FLAGS.batch_size, 1])})\r\n",
        "                    label_keep = []\r\n",
        "\r\n",
        "                    ###############################\r\n",
        "                    hard_margin = 10\r\n",
        "\r\n",
        "                    # Max-Min distance in genuines\r\n",
        "                    max_gen = 0\r\n",
        "                    min_gen = 100\r\n",
        "                    for j in range(label_train.shape[0]):\r\n",
        "                        if label_train[j] == 1:\r\n",
        "                            if max_gen < distance[j, 0]:\r\n",
        "                                max_gen = distance[j, 0]\r\n",
        "                            if min_gen > distance[j, 0]:\r\n",
        "                                min_gen = distance[j, 0]\r\n",
        "\r\n",
        "                    # Min-Max distance in impostors\r\n",
        "                    min_imp = 100\r\n",
        "                    max_imp = 0\r\n",
        "                    for k in range(label_train.shape[0]):\r\n",
        "                        if label_train[k] == 0:\r\n",
        "                            if min_imp > distance[k, 0]:\r\n",
        "                                min_imp = distance[k, 0]\r\n",
        "                            if max_imp < distance[k, 0]:\r\n",
        "                                max_imp = distance[k, 0]\r\n",
        "\r\n",
        "                    ### Keeping hard impostors and genuines\r\n",
        "                    for i in range(label_train.shape[0]):\r\n",
        "                        # imposter\r\n",
        "                        if label_train[i] == 0:\r\n",
        "                            if distance[i, 0] < max_gen + hard_margin:\r\n",
        "                                label_keep.append(i)\r\n",
        "                        elif label_train[i] == 1:\r\n",
        "                            # if distance[i, 0] > min_imp - hard_margin:\r\n",
        "                            label_keep.append(i)\r\n",
        "\r\n",
        "                    #### Choosing the pairs ######\r\n",
        "                    speech_train = speech_train[label_keep]\r\n",
        "                    mouth_train = mouth_train[label_keep]\r\n",
        "                    label_train = label_train[label_keep]\r\n",
        "\r\n",
        "                ############################################\r\n",
        "                #### Running the training operation ########\r\n",
        "                _, loss_value, score_dissimilarity, summary, training_step, _ = sess.run(\r\n",
        "                    [train_op, loss, distance_l2, summary_op, global_step, is_training],\r\n",
        "                    feed_dict={is_training: True, margin_imp_tensor: 100,\r\n",
        "                               batch_speech: speech_train, batch_mouth: mouth_train,\r\n",
        "                               batch_labels: label_train.reshape([label_train.shape[0], 1])})\r\n",
        "                summary_writer.add_summary(summary, epoch * num_batches_per_epoch + i)\r\n",
        "\r\n",
        "                # try and error method is used to handle the error due to ROC calculation\r\n",
        "                try:\r\n",
        "                    # Calculation of ROC\r\n",
        "                    EER, AUC, AP, fpr, tpr = calculate_roc.calculate_eer_auc_ap(label_train, score_dissimilarity)\r\n",
        "\r\n",
        "                    if (batch_num + 1) % FLAGS.log_every_n_steps == 0:\r\n",
        "                        print(\"Epoch \" + str(epoch + 1) + \", Minibatch \" + str(\r\n",
        "                            batch_num + 1) + \" of %d \" % num_batches_per_epoch + \", Minibatch Loss= \" + \\\r\n",
        "                              \"{:.6f}\".format(loss_value) + \", EER= \" + \"{:.5f}\".format(EER) + \", AUC= \" + \"{:.5f}\".format(\r\n",
        "                            AUC) + \", AP= \" + \"{:.5f}\".format(AP) + \", contrib = %d pairs\" % label_train.shape[0])\r\n",
        "                except:\r\n",
        "                    print(\"Error: \" ,sys.exc_info()[0])\r\n",
        "                    print(\"No contributing impostor pair!\")\r\n",
        "\r\n",
        "            # Save the model\r\n",
        "            saver.save(sess, FLAGS.train_dir, global_step=training_step)\r\n",
        "\r\n",
        "            ###################################################\r\n",
        "            ############## TEST PER EACH EPOCH ################\r\n",
        "            ###################################################\r\n",
        "            score_dissimilarity_vector = np.zeros((FLAGS.batch_size * num_batches_per_epoch_test, 1))\r\n",
        "            label_vector = np.zeros((FLAGS.batch_size * num_batches_per_epoch_test, 1))\r\n",
        "\r\n",
        "            # Loop over all batches\r\n",
        "            for i in range(num_batches_per_epoch_test):\r\n",
        "                start_idx = i * FLAGS.batch_size\r\n",
        "                end_idx = (i + 1) * FLAGS.batch_size\r\n",
        "                speech_test, mouth_test, label_test = test_data['speech'][start_idx:end_idx], test_data['mouth'][\r\n",
        "                                                                                              start_idx:end_idx], test_label[\r\n",
        "                                                                                                                  start_idx:end_idx]\r\n",
        "\r\n",
        "                # # # Uncomment if standardalization is needed\r\n",
        "                # # mean subtraction if necessary\r\n",
        "                # speech_test = (speech_test - mean_speech) / std_speech\r\n",
        "                # mouth_test = (mouth_test - mean_mouth) / std_mouth\r\n",
        "\r\n",
        "                # Evaluation phase\r\n",
        "                # WARNING: margin_imp_tensor has no effect here but it needs to be there because its tensor required a value to feed in!!\r\n",
        "                loss_value, score_dissimilarity, _ = sess.run([loss, distance_l2, is_training],\r\n",
        "                                                              feed_dict={is_training: False,\r\n",
        "                                                                         margin_imp_tensor: 50,\r\n",
        "                                                                         batch_speech: speech_test,\r\n",
        "                                                                         batch_mouth: mouth_test,\r\n",
        "                                                                         batch_labels: label_test.reshape(\r\n",
        "                                                                             [FLAGS.batch_size, 1])})\r\n",
        "                if (i + 1) % FLAGS.log_every_n_steps == 0:\r\n",
        "                    print(\"TESTING: Epoch \" + str(epoch + 1) + \", Minibatch \" + str(\r\n",
        "                        i + 1) + \" of %d \" % num_batches_per_epoch_test)\r\n",
        "                score_dissimilarity_vector[start_idx:end_idx] = score_dissimilarity\r\n",
        "                label_vector[start_idx:end_idx] = label_test\r\n",
        "\r\n",
        "            ##############################\r\n",
        "            ##### K-fold validation ######\r\n",
        "            ##############################\r\n",
        "            K = 10\r\n",
        "            EER = np.zeros((K, 1))\r\n",
        "            AUC = np.zeros((K, 1))\r\n",
        "            AP = np.zeros((K, 1))\r\n",
        "            batch_k_validation = int(label_vector.shape[0] / float(K))\r\n",
        "\r\n",
        "            for i in range(K):\r\n",
        "                EER[i, :], AUC[i, :], AP[i, :], fpr, tpr = calculate_roc.calculate_eer_auc_ap(\r\n",
        "                    label_vector[i * batch_k_validation:(i + 1) * batch_k_validation],\r\n",
        "                    score_dissimilarity_vector[i * batch_k_validation:(i + 1) * batch_k_validation])\r\n",
        "\r\n",
        "            # Printing Equal Error Rate(EER), Area Under the Curve(AUC) and Average Precision(AP)\r\n",
        "            print(\"TESTING: Epoch \" + str(epoch + 1) + \", EER= \" + str(np.mean(EER, axis=0)) + \", AUC= \" + str(\r\n",
        "                np.mean(AUC, axis=0)) + \", AP= \" + str(np.mean(AP, axis=0)))\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    tf.app.run()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKL1CuRlBnr3"
      },
      "source": [
        "# **Testing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYLpJYnnBhQB"
      },
      "source": [
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import sys\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "from tensorflow.python.ops import control_flow_ops\r\n",
        "from nets import nets_factory\r\n",
        "from auxiliary import losses\r\n",
        "from roc_curve import calculate_roc\r\n",
        "import os\r\n",
        "# import matplotlib.pyplot as plt\r\n",
        "slim = tf.contrib.slim\r\n",
        "\r\n",
        "######################\r\n",
        "# Train Directory #\r\n",
        "######################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'test_dir', 'results/TRAIN_CNN_3D/test_logs',\r\n",
        "    'Directory where checkpoints and event logs are written to.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'checkpoint_dir', os.path.expanduser('~/results/'),\r\n",
        "    'Directory where checkpoints and event logs are written to.')\r\n",
        "\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer('num_clones', 1,\r\n",
        "                            'Number of model clones to deploy.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_boolean('clone_on_cpu', False,\r\n",
        "                            'Use CPUs to deploy clones.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer(\r\n",
        "    'log_every_n_steps', 1,\r\n",
        "    'The frequency with which logs are print.')\r\n",
        "\r\n",
        "\r\n",
        "######################\r\n",
        "# Optimization Flags #\r\n",
        "######################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'weight_decay', 0.00004, 'The weight decay on the model weights.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'optimizer', 'adam',\r\n",
        "    'The name of the optimizer, one of \"adadelta\", \"adagrad\", \"adam\",'\r\n",
        "    '\"ftrl\", \"momentum\", \"sgd\" or \"rmsprop\".')\r\n",
        "\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'adam_beta1', 0.9,\r\n",
        "    'The exponential decay rate for the 1st moment estimates.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'adam_beta2', 0.999,\r\n",
        "    'The exponential decay rate for the 2nd moment estimates.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float('opt_epsilon', 1.0, 'Epsilon term for the optimizer.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'momentum', 0.9,\r\n",
        "    'The momentum for the MomentumOptimizer and RMSPropOptimizer.')\r\n",
        "\r\n",
        "#######################\r\n",
        "# Learning Rate Flags #\r\n",
        "#######################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'learning_rate_decay_type',\r\n",
        "    'exponential',\r\n",
        "    'Specifies how the learning rate is decayed. One of \"fixed\", \"exponential\",'\r\n",
        "    ' or \"polynomial\"')\r\n",
        " \r\n",
        "tf.app.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'end_learning_rate', 0.0001,\r\n",
        "    'The minimal end learning rate used by a polynomial decay learning rate.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'label_smoothing', 0.0, 'The amount of label smoothing.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'learning_rate_decay_factor', 0.94, 'Learning rate decay factor.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'num_epochs_per_decay', 5.0,\r\n",
        "    'Number of epochs after which learning rate decays.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_bool(\r\n",
        "    'sync_replicas', False,\r\n",
        "    'Whether or not to synchronize the replicas during training.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer(\r\n",
        "    'replicas_to_aggregate', 1,\r\n",
        "    'The Number of gradients to collect before updating params.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_float(\r\n",
        "    'moving_average_decay', None,\r\n",
        "    'The decay to use for the moving average.'\r\n",
        "    'If left as None, then moving averages are not used.')\r\n",
        "\r\n",
        "#######################\r\n",
        "# Dataset Flags #\r\n",
        "#######################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'model_speech_name', 'lipread_speech', 'The name of the architecture to train.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'model_mouth_name', 'lipread_mouth', 'The name of the architecture to train.')\r\n",
        "\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer(\r\n",
        "    'batch_size', 128, 'The number of samples in each batch.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_integer(\r\n",
        "    'num_epochs', 20, 'The number of epochs for training.')\r\n",
        "\r\n",
        "\r\n",
        "#####################\r\n",
        "# Fine-Tuning Flags #\r\n",
        "#####################\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'checkpoint_path', None,\r\n",
        "    'The path to a checkpoint from which to fine-tune. ex:/home/user/TRAIN/train_logs')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'checkpoint_exclude_scopes', None,\r\n",
        "    'Comma-separated list of scopes of variables to exclude when restoring'\r\n",
        "    'from a checkpoint. ex: vgg_19/fc8/biases,vgg_19/fc8/weights')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_string(\r\n",
        "    'trainable_scopes', None,\r\n",
        "    'Comma-separated list of scopes to filter the set of variables to train.'\r\n",
        "    'By default, None would train all the variables.')\r\n",
        "\r\n",
        "tf.app.flags.DEFINE_boolean(\r\n",
        "    'ignore_missing_vars', False,\r\n",
        "    'When restoring a checkpoint would ignore missing variables.')\r\n",
        "\r\n",
        "# Store all elemnts in FLAG structure!\r\n",
        "FLAGS = tf.app.flags.FLAGS\r\n",
        "\r\n",
        "\r\n",
        "def _configure_learning_rate(num_samples_per_epoch, global_step):\r\n",
        "    \"\"\"Configures the learning rate.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      num_samples_per_epoch: The number of samples in each epoch of training.\r\n",
        "      global_step: The global_step tensor.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      A `Tensor` representing the learning rate.\r\n",
        "\r\n",
        "    Raises:\r\n",
        "      ValueError: if\r\n",
        "    \"\"\"\r\n",
        "    decay_steps = int(num_samples_per_epoch / FLAGS.batch_size *\r\n",
        "                      FLAGS.num_epochs_per_decay)\r\n",
        "    if FLAGS.sync_replicas:\r\n",
        "        decay_steps /= FLAGS.replicas_to_aggregate\r\n",
        "\r\n",
        "    if FLAGS.learning_rate_decay_type == 'exponential':\r\n",
        "        return tf.train.exponential_decay(FLAGS.learning_rate,\r\n",
        "                                          global_step,\r\n",
        "                                          decay_steps,\r\n",
        "                                          FLAGS.learning_rate_decay_factor,\r\n",
        "                                          staircase=True,\r\n",
        "                                          name='exponential_decay_learning_rate')\r\n",
        "    elif FLAGS.learning_rate_decay_type == 'fixed':\r\n",
        "        return tf.constant(FLAGS.learning_rate, name='fixed_learning_rate')\r\n",
        "    elif FLAGS.learning_rate_decay_type == 'polynomial':\r\n",
        "        return tf.train.polynomial_decay(FLAGS.learning_rate,\r\n",
        "                                         global_step,\r\n",
        "                                         decay_steps,\r\n",
        "                                         FLAGS.end_learning_rate,\r\n",
        "                                         power=1.0,\r\n",
        "                                         cycle=False,\r\n",
        "                                         name='polynomial_decay_learning_rate')\r\n",
        "    else:\r\n",
        "        raise ValueError('learning_rate_decay_type [%s] was not recognized',\r\n",
        "                         FLAGS.learning_rate_decay_type)\r\n",
        "\r\n",
        "\r\n",
        "def _configure_optimizer(learning_rate):\r\n",
        "    \"\"\"Configures the optimizer used for training.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      learning_rate: A scalar or `Tensor` learning rate.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      An instance of an optimizer.\r\n",
        "\r\n",
        "    Raises:\r\n",
        "      ValueError: if FLAGS.optimizer is not recognized.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    if FLAGS.optimizer == 'adam':\r\n",
        "        optimizer = tf.train.AdamOptimizer(\r\n",
        "            learning_rate,\r\n",
        "            beta1=FLAGS.adam_beta1,\r\n",
        "            beta2=FLAGS.adam_beta2,\r\n",
        "            epsilon=FLAGS.opt_epsilon)\r\n",
        "    elif FLAGS.optimizer == 'sgd':\r\n",
        "        optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n",
        "    else:\r\n",
        "        raise ValueError('Optimizer [%s] was not recognized', FLAGS.optimizer)\r\n",
        "    return optimizer\r\n",
        "\r\n",
        "\r\n",
        "def average_gradients(tower_grads):\r\n",
        "    \"\"\"Calculate the average gradient for each shared variable across all towers.\r\n",
        "\r\n",
        "    Note that this function provides a synchronization point across all towers.\r\n",
        "\r\n",
        "    Args:\r\n",
        "      tower_grads: List of lists of (gradient, variable) tuples. The outer list\r\n",
        "        is over individual gradients. The inner list is over the gradient\r\n",
        "        calculation for each tower.\r\n",
        "    Returns:\r\n",
        "       List of pairs of (gradient, variable) where the gradient has been averaged\r\n",
        "       across all towers.\r\n",
        "    \"\"\"\r\n",
        "    average_grads = []\r\n",
        "    for grad_and_vars in zip(*tower_grads):\r\n",
        "        # Note that each grad_and_vars looks like the following:\r\n",
        "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n",
        "        grads = []\r\n",
        "        for g, _ in grad_and_vars:\r\n",
        "            # Add 0 dimension to the gradients to represent the tower.\r\n",
        "            expanded_g = tf.expand_dims(g, 0)\r\n",
        "\r\n",
        "            # Append on a 'tower' dimension which we will average over below.\r\n",
        "            grads.append(expanded_g)\r\n",
        "\r\n",
        "        # Average over the 'tower' dimension.\r\n",
        "        grad = tf.concat(axis=0, values=grads)\r\n",
        "        grad = tf.reduce_mean(grad, 0)\r\n",
        "\r\n",
        "        # Keep in mind that the Variables are redundant because they are shared\r\n",
        "        # across towers. So .. we will just return the first tower's pointer to\r\n",
        "        # the Variable.\r\n",
        "        v = grad_and_vars[0][1]\r\n",
        "        grad_and_var = (grad, v)\r\n",
        "        average_grads.append(grad_and_var)\r\n",
        "    return average_grads\r\n",
        "\r\n",
        "\r\n",
        "def _get_variables_to_train():\r\n",
        "    \"\"\"Returns a list of variables to train.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "      A list of variables to train by the optimizer.\r\n",
        "    \"\"\"\r\n",
        "    if FLAGS.trainable_scopes is None:\r\n",
        "        return tf.trainable_variables()\r\n",
        "    else:\r\n",
        "        scopes = [scope.strip() for scope in FLAGS.trainable_scopes.split(',')]\r\n",
        "\r\n",
        "    variables_to_train = []\r\n",
        "    for scope in scopes:\r\n",
        "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\r\n",
        "        variables_to_train.extend(variables)\r\n",
        "    return variables_to_train\r\n",
        "\r\n",
        "\r\n",
        "# Definign arbitrary data\r\n",
        "num_training_samples = 1000\r\n",
        "num_testing_samples = 1000\r\n",
        "train_data = {}\r\n",
        "\r\n",
        "train_data = {'mouth': np.random.random_sample(size=(num_training_samples, 9, 60, 100, 1)),\r\n",
        "              'speech': np.random.random_sample(size=(num_training_samples, 15, 40, 1, 3))}\r\n",
        "test_data = {'mouth': np.random.random_sample(size=(num_testing_samples, 9, 60, 100, 1)),\r\n",
        "             'speech': np.random.random_sample(size=(num_testing_samples, 15, 40, 1, 3))}\r\n",
        "\r\n",
        "train_label = np.random.randint(2, size=(num_training_samples, 1))\r\n",
        "test_label = np.random.randint(2, size=(num_testing_samples, 1))\r\n",
        "\r\n",
        "\r\n",
        "# # Uncomment if data standardalization is required and the mean and std vectors have been calculated.\r\n",
        "# ############ Get the mean vectors ####################\r\n",
        "#\r\n",
        "# # mean mouth\r\n",
        "# mean_mouth = np.load('/path/to/mean/file/mouth.npy')\r\n",
        "# # mean_mouth = np.tile(mean_mouth.reshape(47, 73, 1), (1, 1, 9))\r\n",
        "# mean_mouth = mean_mouth[None, :]\r\n",
        "# mean_channel_mouth = np.mean(mean_mouth)\r\n",
        "#\r\n",
        "# # mean speech\r\n",
        "# mean_speech = np.load('/path/to/mean/file/speech.npy')\r\n",
        "# mean_speech = mean_speech[None, :]\r\n",
        "# # mean_channel_speech = np.hstack((\r\n",
        "# #     [np.mean(mean_speech[:, :, :, 0])], [np.mean(mean_speech[:, :, :, 1])], [np.mean(mean_speech[:, :, :, 2])]))\r\n",
        "#\r\n",
        "# ############ Get the std vectors ####################\r\n",
        "#\r\n",
        "# # mean std\r\n",
        "# std_mouth = np.load('/path/to/std/file/mouth.npy')\r\n",
        "# std_mouth = np.tile(std_mouth.reshape(60, 100, 1), (1, 1, 9))\r\n",
        "# std_mouth = std_mouth[None, :]\r\n",
        "#\r\n",
        "# # mean speech\r\n",
        "# std_speech = np.load('/path/to/std/file/speech.npy')\r\n",
        "# std_speech = std_speech[None, :]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def main(_):\r\n",
        "\r\n",
        "\r\n",
        "    tf.logging.set_verbosity(tf.logging.INFO)\r\n",
        "\r\n",
        "    graph = tf.Graph()\r\n",
        "    with graph.as_default(), tf.device('/cpu:0'):\r\n",
        "        ######################\r\n",
        "        # Config model_deploy#\r\n",
        "        ######################\r\n",
        "\r\n",
        "        # required from data\r\n",
        "        num_samples_per_epoch = train_data['mouth'].shape[0]\r\n",
        "        num_batches_per_epoch = int(num_samples_per_epoch / FLAGS.batch_size)\r\n",
        "\r\n",
        "        num_samples_per_epoch_test = test_data['mouth'].shape[0]\r\n",
        "        num_batches_per_epoch_test = int(num_samples_per_epoch_test / FLAGS.batch_size)\r\n",
        "\r\n",
        "        # Create global_step\r\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\r\n",
        "\r\n",
        "        #########################################\r\n",
        "        # Configure the larning rate. #\r\n",
        "        #########################################\r\n",
        "        learning_rate = _configure_learning_rate(num_samples_per_epoch, global_step)\r\n",
        "        opt = _configure_optimizer(learning_rate)\r\n",
        "\r\n",
        "        ######################\r\n",
        "        # Select the network #\r\n",
        "        ######################\r\n",
        "        is_training = tf.placeholder(tf.bool)\r\n",
        "\r\n",
        "        network_speech_fn = nets_factory.get_network_fn(\r\n",
        "            FLAGS.model_speech_name,\r\n",
        "            num_classes=2,\r\n",
        "            weight_decay=FLAGS.weight_decay,\r\n",
        "            is_training=is_training)\r\n",
        "\r\n",
        "        network_mouth_fn = nets_factory.get_network_fn(\r\n",
        "            FLAGS.model_mouth_name,\r\n",
        "            num_classes=2,\r\n",
        "            weight_decay=FLAGS.weight_decay,\r\n",
        "            is_training=is_training)\r\n",
        "\r\n",
        "        #####################################\r\n",
        "        # Select the preprocessing function #\r\n",
        "        #####################################\r\n",
        "\r\n",
        "        # TODO: Do some preprocessing if necessary.\r\n",
        "\r\n",
        "        ##############################################################\r\n",
        "        # Create a dataset provider that loads data from the dataset #\r\n",
        "        ##############################################################\r\n",
        "        # with tf.device(deploy_config.inputs_device()):\r\n",
        "        \"\"\"\r\n",
        "        Define the place holders and creating the batch tensor.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # Mouth spatial set\r\n",
        "        INPUT_SEQ_LENGTH = 9\r\n",
        "        INPUT_HEIGHT = 60\r\n",
        "        INPUT_WIDTH = 100\r\n",
        "        INPUT_CHANNELS = 1\r\n",
        "        batch_mouth = tf.placeholder(tf.float32, shape=(\r\n",
        "            [None, INPUT_SEQ_LENGTH, INPUT_HEIGHT, INPUT_WIDTH, INPUT_CHANNELS]))\r\n",
        "\r\n",
        "        # Speech spatial set\r\n",
        "        INPUT_SEQ_LENGTH_SPEECH = 15\r\n",
        "        INPUT_HEIGHT_SPEECH = 40\r\n",
        "        INPUT_WIDTH_SPEECH = 1\r\n",
        "        INPUT_CHANNELS_SPEECH = 3\r\n",
        "        batch_speech = tf.placeholder(tf.float32, shape=(\r\n",
        "            [None, INPUT_SEQ_LENGTH_SPEECH, INPUT_HEIGHT_SPEECH, INPUT_WIDTH_SPEECH, INPUT_CHANNELS_SPEECH]))\r\n",
        "\r\n",
        "        # Label\r\n",
        "        batch_labels = tf.placeholder(tf.uint8, (None, 1))\r\n",
        "        margin_imp_tensor = tf.placeholder(tf.float32, ())\r\n",
        "\r\n",
        "        ################################\r\n",
        "        ## Feed forwarding to network ##\r\n",
        "        ################################\r\n",
        "        tower_grads = []\r\n",
        "        with tf.variable_scope(tf.get_variable_scope()):\r\n",
        "            with tf.device('/gpu:%d' % 0):\r\n",
        "                with tf.name_scope('%s_%d' % ('tower', 0)) as scope:\r\n",
        "                    \"\"\"\r\n",
        "                    Two distance metric are defined:\r\n",
        "                       1 - distance_weighted: which is a weighted average of the distance between two structures.\r\n",
        "                       2 - distance_l2: which is the regular l2-norm of the two networks outputs.\r\n",
        "                    Place holders\r\n",
        "\r\n",
        "                    \"\"\"\r\n",
        "                    ########################################\r\n",
        "                    ######## Outputs of two networks #######\r\n",
        "                    ########################################\r\n",
        "\r\n",
        "                    logits_speech, end_points_speech = network_speech_fn(batch_speech)\r\n",
        "                    logits_mouth, end_points_mouth = network_mouth_fn(batch_mouth)\r\n",
        "\r\n",
        "                    # # Uncomment if the output embedding is desired to be as |f(x)| = 1\r\n",
        "                    # logits_speech = tf.nn.l2_normalize(logits_speech, dim=1, epsilon=1e-12, name=None)\r\n",
        "                    # logits_mouth = tf.nn.l2_normalize(logits_mouth, dim=1, epsilon=1e-12, name=None)\r\n",
        "\r\n",
        "                    #################################################\r\n",
        "                    ########### Loss Calculation ####################\r\n",
        "                    #################################################\r\n",
        "\r\n",
        "                    # ##### Weighted distance using a fully connected layer #####\r\n",
        "                    # distance_vector = tf.subtract(logits_speech, logits_mouth,  name=None)\r\n",
        "                    # distance_weighted = slim.fully_connected(distance_vector, 1, activation_fn=tf.nn.sigmoid,\r\n",
        "                    #                                          normalizer_fn=None,\r\n",
        "                    #                                          scope='fc_weighted')\r\n",
        "\r\n",
        "                    ##### Euclidean distance ####\r\n",
        "                    distance_l2 = tf.sqrt(\r\n",
        "                        tf.reduce_sum(tf.pow(tf.subtract(logits_speech, logits_mouth), 2), 1, keep_dims=True))\r\n",
        "\r\n",
        "                    ##### Contrastive loss ######\r\n",
        "                    loss = losses.contrastive_loss(batch_labels, distance_l2, margin_imp=margin_imp_tensor,\r\n",
        "                                                   scope=scope)\r\n",
        "\r\n",
        "                    # ##### call the optimizer ######\r\n",
        "                    # # TODO: call optimizer object outside of this gpu environment\r\n",
        "                    #\r\n",
        "                    # Reuse variables for the next tower.\r\n",
        "                    tf.get_variable_scope().reuse_variables()\r\n",
        "\r\n",
        "                    # Calculate the gradients for the batch of data on this CIFAR tower.\r\n",
        "                    grads = opt.compute_gradients(loss)\r\n",
        "\r\n",
        "                    # Keep track of the gradients across all towers.\r\n",
        "                    tower_grads.append(grads)\r\n",
        "\r\n",
        "\r\n",
        "        # Calculate the mean of each gradient.\r\n",
        "        grads = average_gradients(tower_grads)\r\n",
        "\r\n",
        "        # Apply the gradients to adjust the shared variables.\r\n",
        "        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\r\n",
        "\r\n",
        "        # Track the moving averages of all trainable variables.\r\n",
        "        MOVING_AVERAGE_DECAY = 0.9999\r\n",
        "        variable_averages = tf.train.ExponentialMovingAverage(\r\n",
        "            MOVING_AVERAGE_DECAY, global_step)\r\n",
        "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\r\n",
        "\r\n",
        "        # Group all updates to into a single train op.\r\n",
        "        train_op = tf.group(apply_gradient_op, variables_averages_op)\r\n",
        "\r\n",
        "        #################################################\r\n",
        "        ########### Summary Section #####################\r\n",
        "        #################################################\r\n",
        "\r\n",
        "        # Gather initial summaries.\r\n",
        "        summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))\r\n",
        "\r\n",
        "        # Add summaries for all end_points.\r\n",
        "        for end_point in end_points_speech:\r\n",
        "            x = end_points_speech[end_point]\r\n",
        "            # summaries.add(tf.summary.histogram('activations_speech/' + end_point, x))\r\n",
        "            summaries.add(tf.summary.scalar('sparsity_speech/' + end_point,\r\n",
        "                                            tf.nn.zero_fraction(x)))\r\n",
        "\r\n",
        "        for end_point in end_points_mouth:\r\n",
        "            x = end_points_mouth[end_point]\r\n",
        "            # summaries.add(tf.summary.histogram('activations_mouth/' + end_point, x))\r\n",
        "            summaries.add(tf.summary.scalar('sparsity_mouth/' + end_point,\r\n",
        "                                            tf.nn.zero_fraction(x)))\r\n",
        "\r\n",
        "        # Add summaries for variables.\r\n",
        "        for variable in slim.get_model_variables():\r\n",
        "            summaries.add(tf.summary.histogram(variable.op.name, variable))\r\n",
        "\r\n",
        "        # Add to parameters to summaries\r\n",
        "        summaries.add(tf.summary.scalar('learning_rate', learning_rate))\r\n",
        "        summaries.add(tf.summary.scalar('eval/Loss', loss))\r\n",
        "        summaries |= set(tf.get_collection(tf.GraphKeys.SUMMARIES))\r\n",
        "\r\n",
        "        # Merge all summaries together.\r\n",
        "        summary_op = tf.summary.merge(list(summaries), name='summary_op')\r\n",
        "\r\n",
        "    ###########################\r\n",
        "    ######## Training #########\r\n",
        "    ###########################\r\n",
        "\r\n",
        "    with tf.Session(graph=graph, config=tf.ConfigProto(allow_soft_placement=True)) as sess:\r\n",
        "\r\n",
        "        # Initialization of the network.\r\n",
        "        variables_to_restore = slim.get_variables_to_restore()\r\n",
        "        saver = tf.train.Saver(variables_to_restore, max_to_keep=20)\r\n",
        "        coord = tf.train.Coordinator()\r\n",
        "        sess.run(tf.global_variables_initializer())\r\n",
        "        sess.run(tf.local_variables_initializer())\r\n",
        "\r\n",
        "        # Restore the model\r\n",
        "        print('Loading from:',FLAGS.checkpoint_dir)\r\n",
        "        latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir=FLAGS.checkpoint_dir)\r\n",
        "        saver.restore(sess, latest_checkpoint)\r\n",
        "\r\n",
        "        # op to write logs to Tensorboard\r\n",
        "        summary_writer = tf.summary.FileWriter(FLAGS.test_dir, graph=graph)\r\n",
        "\r\n",
        "        ###################################################\r\n",
        "        ############################ TEST  ################\r\n",
        "        ###################################################\r\n",
        "        score_dissimilarity_vector = np.zeros((FLAGS.batch_size * num_batches_per_epoch_test, 1))\r\n",
        "        label_vector = np.zeros((FLAGS.batch_size * num_batches_per_epoch_test, 1))\r\n",
        "\r\n",
        "        # Loop over all batches\r\n",
        "        for i in range(num_batches_per_epoch_test):\r\n",
        "            start_idx = i * FLAGS.batch_size\r\n",
        "            end_idx = (i + 1) * FLAGS.batch_size\r\n",
        "            speech_test, mouth_test, label_test = test_data['speech'][start_idx:end_idx], test_data['mouth'][\r\n",
        "                                                                                          start_idx:end_idx], test_label[\r\n",
        "                                                                                                              start_idx:end_idx]\r\n",
        "\r\n",
        "            # # # Uncomment if standardalization is needed\r\n",
        "            # # mean subtraction if necessary\r\n",
        "            # speech_test = (speech_test - mean_speech) / std_speech\r\n",
        "            # mouth_test = (mouth_test - mean_mouth) / std_mouth\r\n",
        "\r\n",
        "            # Evaluation phase\r\n",
        "            # WARNING: margin_imp_tensor has no effect here but it needs to be there because its tensor required a value to feed in!!\r\n",
        "            loss_value, score_dissimilarity, _ = sess.run([loss, distance_l2, is_training],\r\n",
        "                                                          feed_dict={is_training: False,\r\n",
        "                                                                     margin_imp_tensor: 50,\r\n",
        "                                                                     batch_speech: speech_test,\r\n",
        "                                                                     batch_mouth: mouth_test,\r\n",
        "                                                                     batch_labels: label_test.reshape(\r\n",
        "                                                                         [FLAGS.batch_size, 1])})\r\n",
        "            if (i + 1) % FLAGS.log_every_n_steps == 0:\r\n",
        "                print(\"TESTING:\" + \", Minibatch \" + str(\r\n",
        "                    i + 1) + \" of %d \" % num_batches_per_epoch_test)\r\n",
        "            score_dissimilarity_vector[start_idx:end_idx] = score_dissimilarity\r\n",
        "            label_vector[start_idx:end_idx] = label_test\r\n",
        "\r\n",
        "        ##############################\r\n",
        "        ##### K-fold validation ######\r\n",
        "        ##############################\r\n",
        "        K = 10\r\n",
        "        EER = np.zeros((K, 1))\r\n",
        "        AUC = np.zeros((K, 1))\r\n",
        "        AP = np.zeros((K, 1))\r\n",
        "        batch_k_validation = int(label_vector.shape[0] / float(K))\r\n",
        "\r\n",
        "        for i in range(K):\r\n",
        "            EER[i, :], AUC[i, :], AP[i, :], fpr, tpr = calculate_roc.calculate_eer_auc_ap(\r\n",
        "                label_vector[i * batch_k_validation:(i + 1) * batch_k_validation],\r\n",
        "                score_dissimilarity_vector[i * batch_k_validation:(i + 1) * batch_k_validation])\r\n",
        "\r\n",
        "        # Printing Equal Error Rate(EER), Area Under the Curve(AUC) and Average Precision(AP)\r\n",
        "        print(\"TESTING:\" +\", EER= \" + str(np.mean(EER, axis=0)) + \", AUC= \" + str(\r\n",
        "            np.mean(AUC, axis=0)) + \", AP= \" + str(np.mean(AP, axis=0)))\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    tf.app.run()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACVVAUJzBsXo"
      },
      "source": [
        "# **Losses**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6tLXoaeB0To"
      },
      "source": [
        "\"\"\"\r\n",
        "Contrastive cost\r\n",
        "\"\"\"\r\n",
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "\r\n",
        "from tensorflow.contrib.framework.python.ops import add_arg_scope\r\n",
        "from tensorflow.python.framework import ops\r\n",
        "from tensorflow.python.ops import array_ops\r\n",
        "from tensorflow.python.ops import math_ops\r\n",
        "from tensorflow.python.ops import nn\r\n",
        "from tensorflow.python.ops import nn_ops\r\n",
        "from tensorflow.python.util.deprecation import deprecated\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "# def contrastive_loss(onehot_labels, logits, margin=1, scope=None):\r\n",
        "#     \"\"\"With this definition the loss will be calculated.\r\n",
        "#         Args:\r\n",
        "#           y: The labels.\r\n",
        "#           distance: The distance vector between the output features..\r\n",
        "#           batch_size: the batch size is necessary because the loss calculation would be over each batch.\r\n",
        "#         Returns:\r\n",
        "#           The total loss.\r\n",
        "#     \"\"\"\r\n",
        "#     with ops.name_scope(scope, \"contrastive_loss\", [onehot_labels, logits]) as scope:\r\n",
        "#         # logits.get_shape().assert_is_compatible_with(onehot_labels.get_shape())\r\n",
        "#\r\n",
        "#         onehot_labels = math_ops.cast(onehot_labels, logits.dtype)\r\n",
        "#\r\n",
        "#         term_1 = tf.multiply(onehot_labels, tf.square(logits))[:,0:1]\r\n",
        "#         term_2 = tf.multiply(onehot_labels, tf.square(tf.maximum((margin - logits), 0)))[:,1:]\r\n",
        "#\r\n",
        "#         # Contrastive\r\n",
        "#         Contrastive_Loss = tf.add(term_1, term_2) / 2\r\n",
        "#         loss = tf.losses.compute_weighted_loss(Contrastive_Loss, scope=scope)\r\n",
        "#\r\n",
        "#         return tf.losses.compute_weighted_loss(Contrastive_Loss, scope=scope)\r\n",
        "\r\n",
        "def contrastive_loss(labels, logits, margin_gen=0, margin_imp=1, scope=None):\r\n",
        "    \"\"\"With this definition the loss will be calculated.\r\n",
        "        Args:\r\n",
        "          y: The labels.\r\n",
        "          distance: The distance vector between the output features..\r\n",
        "          batch_size: the batch size is necessary because the loss calculation would be over each batch.\r\n",
        "        Returns:\r\n",
        "          The total loss.\r\n",
        "    \"\"\"\r\n",
        "    with ops.name_scope(scope, \"contrastive_loss\", [labels, logits]) as scope:\r\n",
        "        # logits.get_shape().assert_is_compatible_with(onehot_labels.get_shape())\r\n",
        "\r\n",
        "        labels = math_ops.cast(labels, logits.dtype)\r\n",
        "\r\n",
        "        # term_1 = tf.multiply(labels, tf.square(logits))\r\n",
        "        term_1 = tf.multiply(labels, tf.square(tf.maximum((logits - margin_gen), 0)))\r\n",
        "        term_2 = tf.multiply(1 - labels, tf.square(tf.maximum((margin_imp - logits), 0)))\r\n",
        "\r\n",
        "        # Contrastive\r\n",
        "        Contrastive_Loss = tf.add(term_1, term_2) / 2\r\n",
        "        loss = tf.losses.compute_weighted_loss(Contrastive_Loss, scope=scope)\r\n",
        "\r\n",
        "        return loss\r\n",
        "\r\n",
        "\r\n",
        "# def contrastive_loss(onehot_labels, logits, batch_size, margin=1):\r\n",
        "#     \"\"\"With this definition the loss will be calculated.\r\n",
        "#         Args:\r\n",
        "#           y: The labels.\r\n",
        "#           distance: The distance vector between the output features..\r\n",
        "#           batch_size: the batch size is necessary because the loss calculation would be over each batch.\r\n",
        "#         Returns:\r\n",
        "#           The total loss.\r\n",
        "#     \"\"\"\r\n",
        "#     with ops.name_scope(scope, \"contrastive_loss\", [onehot_labels, logits]) as scope:\r\n",
        "#         logits.get_shape().assert_is_compatible_with(onehot_labels.get_shape())\r\n",
        "#\r\n",
        "#         onehot_labels = math_ops.cast(onehot_labels, logits.dtype)\r\n",
        "#\r\n",
        "#         term_1 = tf.multiply(onehot_labels, tf.square(distance))[:,0:1]\r\n",
        "#         term_2 = tf.multiply(onehot_labels, tf.square(tf.maximum((margin - distance), 0)))[:,1:]\r\n",
        "#\r\n",
        "#         # Contrastive\r\n",
        "#         Contrastive_Loss = tf.add(term_1, term_2) / batch_size / 2\r\n",
        "#         tf.add_to_collection('losses', Contrastive_Loss)\r\n",
        "#\r\n",
        "#         return tf.add_n(tf.get_collection('losses'), name='total_loss')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IlBzZ6KB80v"
      },
      "source": [
        "# **Lipread mouth**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkwjTpxyCGr9"
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "# ==============================================================================\r\n",
        "\"\"\"Contains model definitions for versions of the Oxford VGG network.\r\n",
        "\r\n",
        "These model definitions were introduced in the following technical report:\r\n",
        "\r\n",
        "  Very Deep Convolutional Networks For Large-Scale Image Recognition\r\n",
        "  Karen Simonyan and Andrew Zisserman\r\n",
        "  arXiv technical report, 2015\r\n",
        "  PDF: http://arxiv.org/pdf/1409.1556.pdf\r\n",
        "  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\r\n",
        "  CC-BY-4.0\r\n",
        "\r\n",
        "More information can be obtained from the VGG website:\r\n",
        "www.robots.ox.ac.uk/~vgg/research/very_deep/\r\n",
        "\r\n",
        "Usage:\r\n",
        "  with slim.arg_scope(vgg.vgg_arg_scope()):\r\n",
        "    outputs, end_points = vgg.vgg_a(inputs)\r\n",
        "\r\n",
        "  with slim.arg_scope(vgg.vgg_arg_scope()):\r\n",
        "    outputs, end_points = vgg.vgg_16(inputs)\r\n",
        "\r\n",
        "@@vgg_a\r\n",
        "@@vgg_16\r\n",
        "@@vgg_19\r\n",
        "\"\"\"\r\n",
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "slim = tf.contrib.slim\r\n",
        "LSTM_status = False\r\n",
        "\r\n",
        "\r\n",
        "def lipread_mouth_arg_scope(is_training, weight_decay=0.0005):\r\n",
        "  \"\"\"Defines the VGG arg scope.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    weight_decay: The l2 regularization coefficient.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    An arg_scope.\r\n",
        "  \"\"\"\r\n",
        "  # Add normalizer_fn=slim.batch_norm if Batch Normalization is required!\r\n",
        "  with slim.arg_scope([slim.conv3d, slim.fully_connected],\r\n",
        "                      activation_fn=None,\r\n",
        "                      weights_initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG'),\r\n",
        "                      weights_regularizer=slim.l2_regularizer(weight_decay),\r\n",
        "                      normalizer_fn=slim.batch_norm,\r\n",
        "                      biases_initializer=tf.zeros_initializer()):\r\n",
        "    with slim.arg_scope([slim.conv3d], padding='SAME') as arg_sc:\r\n",
        "      return arg_sc\r\n",
        "\r\n",
        "def PReLU(input,scope):\r\n",
        "  \"\"\"\r\n",
        "  Similar to TFlearn implementation\r\n",
        "  :param input: input of the PReLU which is output of a layer.\r\n",
        "  :return: The output.\r\n",
        "  \"\"\"\r\n",
        "  alphas = tf.get_variable(scope, input.get_shape()[-1],\r\n",
        "                       initializer=tf.constant_initializer(0.0),\r\n",
        "                        dtype=tf.float32)\r\n",
        "\r\n",
        "  return tf.nn.relu(input) + alphas * (input - abs(input)) * 0.5\r\n",
        "\r\n",
        "\r\n",
        "def mouth_cnn_lstm(inputs,\r\n",
        "          num_classes=1000,\r\n",
        "          is_training=True,\r\n",
        "          dropout_keep_prob=0.8,\r\n",
        "          spatial_squeeze=True,\r\n",
        "          scope='mouth_cnn'):\r\n",
        "  \"\"\"Oxford Net VGG 11-Layers version A Example.\r\n",
        "\r\n",
        "  Note: All the fully_connected layers have been transformed to conv3d layers.\r\n",
        "        To use in classification mode, resize input to 224x224.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    inputs: a tensor of size [batch_size, height, width, channels].\r\n",
        "    num_classes: number of predicted classes.\r\n",
        "    is_training: whether or not the model is being trained.\r\n",
        "    dropout_keep_prob: the probability that activations are kept in the dropout\r\n",
        "      layers during training.\r\n",
        "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\r\n",
        "      outputs. Useful to remove unnecessary dimensions for classification.\r\n",
        "    scope: Optional scope for the variables.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    the last op containing the log predictions and end_points dict.\r\n",
        "  \"\"\"\r\n",
        "\r\n",
        "  end_points = {}\r\n",
        "  with tf.variable_scope(scope, 'mouth_cnn', [inputs]) as sc:\r\n",
        "    # end_points_collection = sc.name + '_end_points'\r\n",
        "    # # Collect outputs for conv3d, fully_connected and max_pool2d.\r\n",
        "    # with slim.arg_scope([slim.conv3d, slim.max_pool2d],\r\n",
        "    #                     outputs_collections=end_points_collection):\r\n",
        "\r\n",
        "    ##### Convolution Section #####\r\n",
        "    # Tensor(\"batch:1\", shape=(?, 9, 60, 100, 1), dtype=float32, device=/device:CPU:0)\r\n",
        "    inputs = tf.to_float(inputs)\r\n",
        "    net = slim.repeat(inputs, 1, slim.conv3d, 16, [1, 3, 3], scope='conv1')\r\n",
        "    net = PReLU(net, 'conv1_activation')\r\n",
        "    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 2, 1], ksize=[1, 1, 3, 3, 1],padding='VALID', name='pool1')\r\n",
        "    # net = slim.max_pool2d(net, [3, 3], scope='pool1')\r\n",
        "    net = slim.repeat(net, 1, slim.conv3d, 32, [1, 3, 3], scope='conv2')\r\n",
        "    net = PReLU(net, 'conv2_activation')\r\n",
        "    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 2, 1], ksize=[1, 1, 3, 3, 1], padding='VALID', name='pool2')\r\n",
        "\r\n",
        "    net = slim.conv3d(net, 64, [1, 3, 3], scope='conv31')\r\n",
        "    net = PReLU(net, 'conv31_activation')\r\n",
        "    net = slim.conv3d(net, 64, [1, 3, 3], scope='conv32')\r\n",
        "    net = PReLU(net, 'conv32_activation')\r\n",
        "\r\n",
        "    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 2, 1], ksize=[1, 1, 3, 3, 1], padding='VALID', name='pool3')\r\n",
        "    net = slim.repeat(net, 1, slim.conv3d, 128, [1, 3, 3], scope='conv4')\r\n",
        "    net = PReLU(net, 'conv4_activation')\r\n",
        "    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 2, 1], ksize=[1, 1, 3, 3, 1], padding='VALID', name='pool4')\r\n",
        "\r\n",
        "    ##### FC section #####\r\n",
        "    # Use conv3d instead of fully_connected layers.\r\n",
        "    net = slim.repeat(net, 1, slim.conv3d, 256, [1, 2, 5], padding='VALID', scope='fc5')\r\n",
        "    net = PReLU(net, 'fc5_activation')\r\n",
        "    # net = PReLU(net)\r\n",
        "    # net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\r\n",
        "    #                    scope='dropout5')\r\n",
        "\r\n",
        "\r\n",
        "    if LSTM_status:\r\n",
        "\r\n",
        "      net = slim.conv3d(net, 64, [1, 1, 1], padding='VALID', activation_fn=None, normalizer_fn=None, scope='fc6')\r\n",
        "      net = PReLU(net, 'fc6_activation')\r\n",
        "\r\n",
        "      # Tensor(\"tower_0/speech_cnn/fc6/squeezed:0\", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\r\n",
        "      net = tf.squeeze(net, [2, 3], name='fc6/squeezed')\r\n",
        "\r\n",
        "    else:\r\n",
        "      net = slim.conv3d(net, 64, [9, 1, 1],padding='VALID', activation_fn=None, normalizer_fn=None, scope='fc5')\r\n",
        "\r\n",
        "      # Tensor(\"tower_0/speech_cnn/fc6/squeezed:0\", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\r\n",
        "      net = tf.squeeze(net, [1, 2, 3], name='fc6/squeezed')\r\n",
        "\r\n",
        "    if LSTM_status:\r\n",
        "      ##### LSTM-1 #####\r\n",
        "      # use sequence_length=X_lengths argument in tf.nn.dynamic_rnn if necessary.\r\n",
        "      cell_1 = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\r\n",
        "      outputs, last_states = tf.nn.dynamic_rnn(\r\n",
        "        cell=cell_1,\r\n",
        "        dtype=tf.float32,\r\n",
        "        inputs=net,\r\n",
        "        scope='LSTM-mouth')\r\n",
        "      net = last_states.h\r\n",
        "\r\n",
        "    return net, end_points\r\n",
        "\r\n",
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "# ==============================================================================\r\n",
        "\"\"\"Contains model definitions for versions of the Oxford VGG network.\r\n",
        "\r\n",
        "These model definitions were introduced in the following technical report:\r\n",
        "\r\n",
        "  Very Deep Convolutional Networks For Large-Scale Image Recognition\r\n",
        "  Karen Simonyan and Andrew Zisserman\r\n",
        "  arXiv technical report, 2015\r\n",
        "  PDF: http://arxiv.org/pdf/1409.1556.pdf\r\n",
        "  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\r\n",
        "  CC-BY-4.0\r\n",
        "\r\n",
        "More information can be obtained from the VGG website:\r\n",
        "www.robots.ox.ac.uk/~vgg/research/very_deep/\r\n",
        "\r\n",
        "Usage:\r\n",
        "  with slim.arg_scope(vgg.vgg_arg_scope()):\r\n",
        "    outputs, end_points = vgg.vgg_a(inputs)\r\n",
        "\r\n",
        "  with slim.arg_scope(vgg.vgg_arg_scope()):\r\n",
        "    outputs, end_points = vgg.vgg_16(inputs)\r\n",
        "\r\n",
        "@@vgg_a\r\n",
        "@@vgg_16\r\n",
        "@@vgg_19\r\n",
        "\"\"\"\r\n",
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import sys\r\n",
        "\r\n",
        "slim = tf.contrib.slim\r\n",
        "LSTM_status = False\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def lipread_speech_arg_scope(is_training, weight_decay=0.0005,):\r\n",
        "  \"\"\"Defines the VGG arg scope.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    weight_decay: The l2 regularization coefficient.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    An arg_scope.\r\n",
        "  \"\"\"\r\n",
        "  # Add normalizer_fn=slim.batch_norm if Batch Normalization is required!\r\n",
        "  with slim.arg_scope([slim.conv3d, slim.fully_connected],\r\n",
        "                      activation_fn=None,\r\n",
        "                      weights_initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG'),\r\n",
        "                      weights_regularizer=slim.l2_regularizer(weight_decay),\r\n",
        "                      normalizer_fn=slim.batch_norm,\r\n",
        "                      biases_initializer=tf.zeros_initializer()):\r\n",
        "    with slim.arg_scope([slim.conv3d], padding='VALID') as arg_sc:\r\n",
        "      return arg_sc\r\n",
        "\r\n",
        "def PReLU(input,scope):\r\n",
        "  \"\"\"\r\n",
        "  Similar to TFlearn implementation\r\n",
        "  :param input: input of the PReLU which is output of a layer.\r\n",
        "  :return: The output.\r\n",
        "  \"\"\"\r\n",
        "  alphas = tf.get_variable(scope, input.get_shape()[-1],\r\n",
        "                       initializer=tf.constant_initializer(0.0),\r\n",
        "                        dtype=tf.float32)\r\n",
        "\r\n",
        "  return tf.nn.relu(input) + alphas * (input - abs(input)) * 0.5\r\n",
        "\r\n",
        "end_points = {}\r\n",
        "\r\n",
        "def speech_cnn_lstm(inputs,\r\n",
        "          num_classes=1000,\r\n",
        "          is_training=True,\r\n",
        "          dropout_keep_prob=0.8,\r\n",
        "          spatial_squeeze=True,\r\n",
        "          scope='speech_cnn'):\r\n",
        "  \"\"\"Oxford Net VGG 11-Layers version A Example.\r\n",
        "\r\n",
        "  Note: All the fully_connected layers have been transformed to conv3d layers.\r\n",
        "        To use in classification mode, resize input to 224x224.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    inputs: a tensor of size [batch_size, height, width, channels].\r\n",
        "    num_classes: number of predicted classes.\r\n",
        "    is_training: whether or not the model is being trained.\r\n",
        "    dropout_keep_prob: the probability that activations are kept in the dropout\r\n",
        "      layers during training.\r\n",
        "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\r\n",
        "      outputs. Useful to remove unnecessary dimensions for classification.\r\n",
        "    scope: Optional scope for the variables.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    the last op containing the log predictions and end_points dict.\r\n",
        "  \"\"\"\r\n",
        "  with tf.variable_scope(scope, 'speech_cnn', [inputs]) as sc:\r\n",
        "    ##### CNN part #####\r\n",
        "    # Tensor(\"batch:0\", shape=(?, 15, 40, 1, 3), dtype=float32, device=/device:CPU:0)\r\n",
        "    inputs = tf.to_float(inputs)\r\n",
        "    net = slim.repeat(inputs, 1, slim.conv3d, 16, [1, 5, 1], scope='conv1')\r\n",
        "    net = PReLU(net, 'conv1_activation')\r\n",
        "    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 1, 1], ksize=[1, 1, 2, 1, 1], padding='VALID', name='pool1')\r\n",
        "\r\n",
        "    net = slim.conv3d(net, 32, [1, 4, 1], scope='conv21')\r\n",
        "    net = PReLU(net, 'conv21_activation')\r\n",
        "    net = slim.conv3d(net, 32, [1, 4, 1], scope='conv22')\r\n",
        "    net = PReLU(net, 'conv22_activation')\r\n",
        "    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 1, 1], ksize=[1, 1, 2, 1, 1], padding='VALID', name='pool2')\r\n",
        "\r\n",
        "    net = slim.conv3d(net, 64, [1, 3, 1], scope='conv31')\r\n",
        "    net = PReLU(net, 'conv31_activation')\r\n",
        "    net = slim.conv3d(net, 64, [1, 3, 1], scope='conv32')\r\n",
        "    net = PReLU(net, 'conv32_activation')\r\n",
        "\r\n",
        "    ##### FC part #####\r\n",
        "    # Use conv3d instead of fully_connected layers.\r\n",
        "    net = slim.conv3d(net, 128, [1, 2, 1], padding='VALID', scope='fc4')\r\n",
        "    net = PReLU(net, 'fc4_activation')\r\n",
        "    # net = PReLU(net)\r\n",
        "    # net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\r\n",
        "    #                    scope='dropout4')\r\n",
        "\r\n",
        "    if LSTM_status:\r\n",
        "\r\n",
        "      net = slim.conv3d(net, 64, [1, 1, 1], padding='VALID', activation_fn=None, normalizer_fn=None, scope='fc5')\r\n",
        "      net = PReLU(net, 'fc5_activation')\r\n",
        "\r\n",
        "      # Tensor(\"tower_0/speech_cnn/fc6/squeezed:0\", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\r\n",
        "      net = tf.squeeze(net, [2, 3], name='fc5/squeezed')\r\n",
        "\r\n",
        "    else:\r\n",
        "      net = slim.conv3d(net, 64, [15, 1, 1],padding='VALID', activation_fn=None, normalizer_fn=None, scope='fc5')\r\n",
        "\r\n",
        "      # Tensor(\"tower_0/speech_cnn/fc6/squeezed:0\", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\r\n",
        "      net = tf.squeeze(net, [1, 2, 3], name='fc5/squeezed')\r\n",
        "\r\n",
        "    if LSTM_status:\r\n",
        "      ##### LSTM-1 #####\r\n",
        "      # use sequence_length=X_lengths argument in tf.nn.dynamic_rnn if necessary.\r\n",
        "      cell_1 = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\r\n",
        "      outputs, last_states = tf.nn.dynamic_rnn(\r\n",
        "        cell=cell_1,\r\n",
        "        dtype=tf.float32,\r\n",
        "        inputs=net,\r\n",
        "        scope='LSTM-speech')\r\n",
        "      net = last_states.h\r\n",
        "\r\n",
        "    return net, end_points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ysj6u_8CRji"
      },
      "source": [
        "# **Lipread Speech**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nYelCzHCckR"
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "# ==============================================================================\r\n",
        "\"\"\"Contains model definitions for versions of the Oxford VGG network.\r\n",
        "\r\n",
        "These model definitions were introduced in the following technical report:\r\n",
        "\r\n",
        "  Very Deep Convolutional Networks For Large-Scale Image Recognition\r\n",
        "  Karen Simonyan and Andrew Zisserman\r\n",
        "  arXiv technical report, 2015\r\n",
        "  PDF: http://arxiv.org/pdf/1409.1556.pdf\r\n",
        "  ILSVRC 2014 Slides: http://www.robots.ox.ac.uk/~karen/pdf/ILSVRC_2014.pdf\r\n",
        "  CC-BY-4.0\r\n",
        "\r\n",
        "More information can be obtained from the VGG website:\r\n",
        "www.robots.ox.ac.uk/~vgg/research/very_deep/\r\n",
        "\r\n",
        "Usage:\r\n",
        "  with slim.arg_scope(vgg.vgg_arg_scope()):\r\n",
        "    outputs, end_points = vgg.vgg_a(inputs)\r\n",
        "\r\n",
        "  with slim.arg_scope(vgg.vgg_arg_scope()):\r\n",
        "    outputs, end_points = vgg.vgg_16(inputs)\r\n",
        "\r\n",
        "@@vgg_a\r\n",
        "@@vgg_16\r\n",
        "@@vgg_19\r\n",
        "\"\"\"\r\n",
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "import sys\r\n",
        "\r\n",
        "slim = tf.contrib.slim\r\n",
        "LSTM_status = False\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def lipread_speech_arg_scope(is_training, weight_decay=0.0005,):\r\n",
        "  \"\"\"Defines the VGG arg scope.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    weight_decay: The l2 regularization coefficient.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    An arg_scope.\r\n",
        "  \"\"\"\r\n",
        "  # Add normalizer_fn=slim.batch_norm if Batch Normalization is required!\r\n",
        "  with slim.arg_scope([slim.conv3d, slim.fully_connected],\r\n",
        "                      activation_fn=None,\r\n",
        "                      weights_initializer=tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_AVG'),\r\n",
        "                      weights_regularizer=slim.l2_regularizer(weight_decay),\r\n",
        "                      normalizer_fn=slim.batch_norm,\r\n",
        "                      biases_initializer=tf.zeros_initializer()):\r\n",
        "    with slim.arg_scope([slim.conv3d], padding='VALID') as arg_sc:\r\n",
        "      return arg_sc\r\n",
        "\r\n",
        "def PReLU(input,scope):\r\n",
        "  \"\"\"\r\n",
        "  Similar to TFlearn implementation\r\n",
        "  :param input: input of the PReLU which is output of a layer.\r\n",
        "  :return: The output.\r\n",
        "  \"\"\"\r\n",
        "  alphas = tf.get_variable(scope, input.get_shape()[-1],\r\n",
        "                       initializer=tf.constant_initializer(0.0),\r\n",
        "                        dtype=tf.float32)\r\n",
        "\r\n",
        "  return tf.nn.relu(input) + alphas * (input - abs(input)) * 0.5\r\n",
        "\r\n",
        "end_points = {}\r\n",
        "\r\n",
        "def speech_cnn_lstm(inputs,\r\n",
        "          num_classes=1000,\r\n",
        "          is_training=True,\r\n",
        "          dropout_keep_prob=0.8,\r\n",
        "          spatial_squeeze=True,\r\n",
        "          scope='speech_cnn'):\r\n",
        "  \"\"\"Oxford Net VGG 11-Layers version A Example.\r\n",
        "\r\n",
        "  Note: All the fully_connected layers have been transformed to conv3d layers.\r\n",
        "        To use in classification mode, resize input to 224x224.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    inputs: a tensor of size [batch_size, height, width, channels].\r\n",
        "    num_classes: number of predicted classes.\r\n",
        "    is_training: whether or not the model is being trained.\r\n",
        "    dropout_keep_prob: the probability that activations are kept in the dropout\r\n",
        "      layers during training.\r\n",
        "    spatial_squeeze: whether or not should squeeze the spatial dimensions of the\r\n",
        "      outputs. Useful to remove unnecessary dimensions for classification.\r\n",
        "    scope: Optional scope for the variables.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    the last op containing the log predictions and end_points dict.\r\n",
        "  \"\"\"\r\n",
        "  with tf.variable_scope(scope, 'speech_cnn', [inputs]) as sc:\r\n",
        "    ##### CNN part #####\r\n",
        "    # Tensor(\"batch:0\", shape=(?, 15, 40, 1, 3), dtype=float32, device=/device:CPU:0)\r\n",
        "    inputs = tf.to_float(inputs)\r\n",
        "    net = slim.repeat(inputs, 1, slim.conv3d, 16, [1, 5, 1], scope='conv1')\r\n",
        "    net = PReLU(net, 'conv1_activation')\r\n",
        "    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 1, 1], ksize=[1, 1, 2, 1, 1], padding='VALID', name='pool1')\r\n",
        "\r\n",
        "    net = slim.conv3d(net, 32, [1, 4, 1], scope='conv21')\r\n",
        "    net = PReLU(net, 'conv21_activation')\r\n",
        "    net = slim.conv3d(net, 32, [1, 4, 1], scope='conv22')\r\n",
        "    net = PReLU(net, 'conv22_activation')\r\n",
        "    net = tf.nn.max_pool3d(net, strides=[1, 1, 2, 1, 1], ksize=[1, 1, 2, 1, 1], padding='VALID', name='pool2')\r\n",
        "\r\n",
        "    net = slim.conv3d(net, 64, [1, 3, 1], scope='conv31')\r\n",
        "    net = PReLU(net, 'conv31_activation')\r\n",
        "    net = slim.conv3d(net, 64, [1, 3, 1], scope='conv32')\r\n",
        "    net = PReLU(net, 'conv32_activation')\r\n",
        "\r\n",
        "    ##### FC part #####\r\n",
        "    # Use conv3d instead of fully_connected layers.\r\n",
        "    net = slim.conv3d(net, 128, [1, 2, 1], padding='VALID', scope='fc4')\r\n",
        "    net = PReLU(net, 'fc4_activation')\r\n",
        "    # net = PReLU(net)\r\n",
        "    # net = slim.dropout(net, dropout_keep_prob, is_training=is_training,\r\n",
        "    #                    scope='dropout4')\r\n",
        "\r\n",
        "    if LSTM_status:\r\n",
        "\r\n",
        "      net = slim.conv3d(net, 64, [1, 1, 1], padding='VALID', activation_fn=None, normalizer_fn=None, scope='fc5')\r\n",
        "      net = PReLU(net, 'fc5_activation')\r\n",
        "\r\n",
        "      # Tensor(\"tower_0/speech_cnn/fc6/squeezed:0\", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\r\n",
        "      net = tf.squeeze(net, [2, 3], name='fc5/squeezed')\r\n",
        "\r\n",
        "    else:\r\n",
        "      net = slim.conv3d(net, 64, [15, 1, 1],padding='VALID', activation_fn=None, normalizer_fn=None, scope='fc5')\r\n",
        "\r\n",
        "      # Tensor(\"tower_0/speech_cnn/fc6/squeezed:0\", shape=(?, 9, 128), dtype=float32, device=/device:GPU:0)\r\n",
        "      net = tf.squeeze(net, [1, 2, 3], name='fc5/squeezed')\r\n",
        "\r\n",
        "    if LSTM_status:\r\n",
        "      ##### LSTM-1 #####\r\n",
        "      # use sequence_length=X_lengths argument in tf.nn.dynamic_rnn if necessary.\r\n",
        "      cell_1 = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_units=128, state_is_tuple=True)\r\n",
        "      outputs, last_states = tf.nn.dynamic_rnn(\r\n",
        "        cell=cell_1,\r\n",
        "        dtype=tf.float32,\r\n",
        "        inputs=net,\r\n",
        "        scope='LSTM-speech')\r\n",
        "      net = last_states.h\r\n",
        "\r\n",
        "    return net, end_points\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6ai3bouChiJ"
      },
      "source": [
        "# **Nets factory**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWlA2aeMCoG5"
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\r\n",
        "#\r\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\r\n",
        "# you may not use this file except in compliance with the License.\r\n",
        "# You may obtain a copy of the License at\r\n",
        "#\r\n",
        "# http://www.apache.org/licenses/LICENSE-2.0\r\n",
        "#\r\n",
        "# Unless required by applicable law or agreed to in writing, software\r\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\r\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n",
        "# See the License for the specific language governing permissions and\r\n",
        "# limitations under the License.\r\n",
        "# ==============================================================================\r\n",
        "\"\"\"Contains a factory for building various models.\"\"\"\r\n",
        "\r\n",
        "from __future__ import absolute_import\r\n",
        "from __future__ import division\r\n",
        "from __future__ import print_function\r\n",
        "import functools\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from nets import lipread_mouth\r\n",
        "from nets import lipread_speech\r\n",
        "\r\n",
        "slim = tf.contrib.slim\r\n",
        "\r\n",
        "networks_map = {'lipread_mouth':lipread_mouth.mouth_cnn_lstm,\r\n",
        "                'lipread_speech':lipread_speech.speech_cnn_lstm,\r\n",
        "\r\n",
        "               }\r\n",
        "\r\n",
        "arg_scopes_map = {'lipread_mouth':lipread_mouth.lipread_mouth_arg_scope,\r\n",
        "                  'lipread_speech':lipread_speech.lipread_speech_arg_scope,\r\n",
        "                 }\r\n",
        "\r\n",
        "\r\n",
        "def get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):\r\n",
        "  \"\"\"Returns a network_fn such as `logits, end_points = network_fn(images)`.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    name: The name of the network.\r\n",
        "    num_classes: The number of classes to use for classification.\r\n",
        "    weight_decay: The l2 coefficient for the model weights.\r\n",
        "    is_training: `True` if the model is being used for training and `False`\r\n",
        "      otherwise.\r\n",
        "\r\n",
        "  Returns:\r\n",
        "    network_fn: A function that applies the model to a batch of images. It has\r\n",
        "      the following signature:\r\n",
        "        logits, end_points = network_fn(images)\r\n",
        "  Raises:\r\n",
        "    ValueError: If network `name` is not recognized.\r\n",
        "  \"\"\"\r\n",
        "  if name not in networks_map:\r\n",
        "    raise ValueError('Name of network unknown %s' % name)\r\n",
        "\r\n",
        "  func = networks_map[name]\r\n",
        "  @functools.wraps(func)\r\n",
        "  def network_fn(images):\r\n",
        "    arg_scope = arg_scopes_map[name](is_training, weight_decay=weight_decay)\r\n",
        "    with slim.arg_scope(arg_scope):\r\n",
        "      return func(images, num_classes, is_training=is_training)\r\n",
        "  if hasattr(func, 'default_image_size'):\r\n",
        "    network_fn.default_image_size = func.default_image_size\r\n",
        "\r\n",
        "  return network_fn\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdMKbZ2ZCpUK"
      },
      "source": [
        "# **Siamese Architecture for face recognition**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwDuyBqHCwW3"
      },
      "source": [
        "# Siamese Architecture for face recognition\r\n",
        "\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "import math\r\n",
        "import pdb\r\n",
        "import sys\r\n",
        "import scipy.io as sio\r\n",
        "from sklearn import *\r\n",
        "\r\n",
        "def calculate_eer_auc_ap(label,distance):\r\n",
        "\r\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(label, -distance, pos_label=1)\r\n",
        "    AUC = metrics.roc_auc_score(label, -distance, average='macro', sample_weight=None)\r\n",
        "    AP = metrics.average_precision_score(label, -distance, average='macro', sample_weight=None)\r\n",
        "\r\n",
        "    # Calculating EER\r\n",
        "    intersect_x = fpr[np.abs(fpr - (1 - tpr)).argmin(0)]\r\n",
        "    EER = intersect_x\r\n",
        "\r\n",
        "    return EER,AUC,AP,fpr, tpr\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khVJH_djDASB"
      },
      "source": [
        "# Siamese Architecture for face recognition\r\n",
        "\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "import math\r\n",
        "import pdb\r\n",
        "import sys\r\n",
        "import scipy.io as sio\r\n",
        "from sklearn import *\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def Plot_HIST_Fn(label,distance, phase, num_bins = 50):\r\n",
        "\r\n",
        "    dissimilarity = distance[:]\r\n",
        "    gen_dissimilarity_original = []\r\n",
        "    imp_dissimilarity_original = []\r\n",
        "    for i in range(len(label)):\r\n",
        "        if label[i] == 1:\r\n",
        "            gen_dissimilarity_original.append(dissimilarity[i])\r\n",
        "        else:\r\n",
        "            imp_dissimilarity_original.append(dissimilarity[i])\r\n",
        "\r\n",
        "    bins = np.linspace(np.amin(distance), np.amax(distance), num_bins)\r\n",
        "    fig = plt.figure()\r\n",
        "    plt.hist(gen_dissimilarity_original, bins, alpha=0.5, facecolor='blue', normed=False, label='gen_dist_original')\r\n",
        "    plt.hist(imp_dissimilarity_original, bins, alpha=0.5, facecolor='red', normed=False, label='imp_dist_original')\r\n",
        "    plt.legend(loc='upper right')\r\n",
        "    plt.title(phase + '_' + 'OriginalFeatures_Histogram.jpg')\r\n",
        "    plt.show()\r\n",
        "    fig.savefig(phase + '_' + 'OriginalFeatures_Histogram.jpg')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjjeGqjmDE16"
      },
      "source": [
        "# Siamese Architecture for face recognition\r\n",
        "\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "import math\r\n",
        "import pdb\r\n",
        "import sys\r\n",
        "import scipy.io as sio\r\n",
        "from sklearn import *\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def Plot_PR_Fn(label,distance,phase):\r\n",
        "\r\n",
        "    precision, recall, thresholds = metrics.precision_recall_curve(label, -distance, pos_label=1, sample_weight=None)\r\n",
        "    AP = metrics.average_precision_score(label, -distance, average='macro', sample_weight=None)\r\n",
        "\r\n",
        "    # AP(average precision) calculation.\r\n",
        "    # This score corresponds to the area under the precision-recall curve.\r\n",
        "    print(\"AP = \", float((\"{0:.%ie}\" % 1).format(AP)))\r\n",
        "\r\n",
        "    # Plot the ROC\r\n",
        "    fig = plt.figure()\r\n",
        "    ax = fig.gca()\r\n",
        "    lines = plt.plot(recall, precision, label='ROC Curve')\r\n",
        "    plt.setp(lines, linewidth=2, color='r')\r\n",
        "    ax.set_xticks(np.arange(0, 1.1, 0.1))\r\n",
        "    ax.set_yticks(np.arange(0, 1.1, 0.1))\r\n",
        "    plt.title(phase + '_' + 'PR.jpg')\r\n",
        "    plt.xlabel('Recall')\r\n",
        "    plt.ylabel('Precision')\r\n",
        "\r\n",
        "    # Cutting the floating number\r\n",
        "    AP = '%.2f' % AP\r\n",
        "\r\n",
        "    # Setting text to plot\r\n",
        "    # plt.text(0.5, 0.5, 'AP = ' + str(AP), fontdict=None)\r\n",
        "    plt.grid()\r\n",
        "    plt.show()\r\n",
        "    fig.savefig(phase + '_' + 'PR.jpg')\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZaLr-4ODI5i"
      },
      "source": [
        "# Siamese Architecture for face recognition\r\n",
        "\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import time\r\n",
        "import tensorflow as tf\r\n",
        "import math\r\n",
        "import pdb\r\n",
        "import sys\r\n",
        "import scipy.io as sio\r\n",
        "from sklearn import *\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "def Plot_ROC_Fn(label,distance,phase):\r\n",
        "\r\n",
        "    fpr, tpr, thresholds = metrics.roc_curve(label, -distance, pos_label=1)\r\n",
        "    AUC = metrics.roc_auc_score(label, -distance, average='macro', sample_weight=None)\r\n",
        "    # AP = metrics.average_precision_score(label, -distance, average='macro', sample_weight=None)\r\n",
        "\r\n",
        "    # Calculating EER\r\n",
        "    intersect_x = fpr[np.abs(fpr - (1 - tpr)).argmin(0)]\r\n",
        "    EER = intersect_x\r\n",
        "    print(\"EER = \", float((\"{0:.%ie}\" % 1).format(intersect_x)))\r\n",
        "\r\n",
        "    # AUC(area under the curve) calculation\r\n",
        "    print(\"AUC = \", float((\"{0:.%ie}\" % 1).format(AUC)))\r\n",
        "\r\n",
        "    # # AP(average precision) calculation.\r\n",
        "    # # This score corresponds to the area under the precision-recall curve.\r\n",
        "    # print(\"AP = \", float((\"{0:.%ie}\" % 1).format(AP)))\r\n",
        "\r\n",
        "    # Plot the ROC\r\n",
        "    fig = plt.figure()\r\n",
        "    ax = fig.gca()\r\n",
        "    lines = plt.plot(fpr, tpr, label='ROC Curve')\r\n",
        "    plt.setp(lines, linewidth=2, color='r')\r\n",
        "    ax.set_xticks(np.arange(0, 1.1, 0.1))\r\n",
        "    ax.set_yticks(np.arange(0, 1.1, 0.1))\r\n",
        "    plt.title(phase + '_' + 'ROC.jpg')\r\n",
        "    plt.xlabel('False Positive Rate')\r\n",
        "    plt.ylabel('True Positive Rate')\r\n",
        "\r\n",
        "    # # Cutting the floating number\r\n",
        "    # AUC = '%.2f' % AUC\r\n",
        "    # EER = '%.2f' % EER\r\n",
        "    # # AP = '%.2f' % AP\r\n",
        "    #\r\n",
        "    # # Setting text to plot\r\n",
        "    # # plt.text(0.5, 0.6, 'AP = ' + str(AP), fontdict=None)\r\n",
        "    # plt.text(0.5, 0.5, 'AUC = ' + str(AUC), fontdict=None)\r\n",
        "    # plt.text(0.5, 0.4, 'EER = ' + str(EER), fontdict=None)\r\n",
        "    plt.grid()\r\n",
        "    plt.show()\r\n",
        "    fig.savefig(phase + '_' + 'ROC.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM9Xq-mLDKVj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}